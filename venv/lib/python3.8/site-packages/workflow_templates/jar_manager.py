from __future__ import print_function
from builtins import object
from argparse import Namespace
import os
import subprocess
import abc
from collections import namedtuple
import uritools
from etsy_dataproc_common_config import jars
import enum
from future.utils import with_metaclass

class JarModes(enum.Enum):
    """ Define the allowed jar modes and their command-line arguments """
    LATEST_PRODUCTION_BUILD = '--with_latest_production_build_jar'
    LATEST_NIGHTLY = '--with_latest_nightly_jar'
    LATEST_SPARK_BUILD = '--with_latest_spark_build'
    LOCALLY_BUILT = '--with_locally_built_jar'
    LOCALLY_BUILT_SPARK = '--with_locally_built_spark_jar'
    DATED_NIGHTLY = '--with_nightly_jar_from_date'
    LATEST_FROM_BRANCH = '--with_latest_jar_from_branch'
    FIXED_URI = '--with_fixed_jar_uri'

# restrict the set of modes that can be specified as default modes.
# we allow any mode that does not require additional user-defined
# arguments.
ALLOWED_DEFAULT_JAR_MODES = frozenset([
    JarModes.LATEST_PRODUCTION_BUILD,
    JarModes.LATEST_NIGHTLY,
    JarModes.LOCALLY_BUILT,
    ])

def comma_separated(item):
    # for parsing comma-separated lists on the command line
    return item.split(',')


class JarManager(with_metaclass(abc.ABCMeta, object)):

    @abc.abstractmethod
    def get_primary_jar(self):
        pass


    @property
    def jars(self):
        primary_jar = self.get_primary_jar()
        print('Using job jar at URI: {}'.format(primary_jar))
        return [primary_jar] + self.extra_jars


    def __init__(self, user, job_name, extra_jars, stallion_config, transport = None):
        self.user = user
        self.job_name = job_name

        self.extra_jars = extra_jars or []
        for jar in self.extra_jars:
            self.check_gcs_uri(jar)

        self.stallion_config = stallion_config
        self.transport = transport or jars.GcsTransport()


    @staticmethod
    def configure_parser(parser, allowed_modes=None, add_extra_jars_arg=True):
        jar_group = parser.add_argument_group('Jar file configuration')

        allowed_modes = frozenset(allowed_modes or JarModes)
        bad_modes = [ mode for mode in allowed_modes if mode not in JarModes ]
        if bad_modes:
            raise Exception(
                'Invalid jar modes specified for configure_parser: {}'.format(
                    bad_modes))

        jar_setting = jar_group.add_mutually_exclusive_group()

        if JarModes.LOCALLY_BUILT in allowed_modes:
            jar_setting.add_argument(
                    JarModes.LOCALLY_BUILT.value,
                    help='Optional. Build a jar locally and use this for running the job',
                    action='store_true')

        if JarModes.LOCALLY_BUILT_SPARK in allowed_modes:
            jar_setting.add_argument(
                    JarModes.LOCALLY_BUILT_SPARK.value,
                    help='Optional. Build a local jar of the specified module and use this for running the job')

        if (JarModes.LOCALLY_BUILT in allowed_modes
                or JarModes.LOCALLY_BUILT_SPARK in allowed_modes):
            jar_group.add_argument(
                '--use_cached_jars', action='store_true', default=False,
                help='Optional. Cache locally-built jars.  This setting is only valid when using locally-built jars')

            jar_group.add_argument(
                '--nobuild', action='store_true', default=False,
                help='Optional. Do not build any jars locally, upload existing jars.  This setting is only valid when using locally-built jars'),

        if JarModes.LATEST_PRODUCTION_BUILD in allowed_modes:
            jar_setting.add_argument(
                    JarModes.LATEST_PRODUCTION_BUILD.value,
                    help='Optional. Use the most-recently-built production jar',
                    action='store_true')

        if JarModes.LATEST_NIGHTLY in allowed_modes:
            jar_setting.add_argument(
                    JarModes.LATEST_NIGHTLY.value,
                    help='Optional. Use the jar used by the most-recently nightly job run',
                    action='store_true')

        if JarModes.LATEST_SPARK_BUILD in allowed_modes:
            jar_setting.add_argument(
                    JarModes.LATEST_SPARK_BUILD.value,
                    help='Optional. Use the most-recently-built Spark production jar of the specified module')

        if JarModes.DATED_NIGHTLY in allowed_modes:
            jar_setting.add_argument(
                    JarModes.DATED_NIGHTLY.value,
                    help='Optional. Use the jar used by the nightly job run on the specified date')

        if JarModes.LATEST_FROM_BRANCH in allowed_modes:
            jar_setting.add_argument(
                    JarModes.LATEST_FROM_BRANCH.value,
                    help='Optional. Use the most-recently-built jar from the specified feature branch')

        if JarModes.FIXED_URI in allowed_modes:
            jar_setting.add_argument(
                    JarModes.FIXED_URI.value,
                    help='Optional. Use the specific jar URI provided')

        if add_extra_jars_arg:
            jar_group.add_argument(
                    '--with_extra_jars',
                    type=comma_separated,
                    default=[],
                    help='Optional. Extra JARs to include in the job submission.  Must be a comma-separated list of GCS URIs.',
                    )

        return parser


    def get_jar_util(self):
        config_jars = self.stallion_config.get('jars', {})

        return jars.GcsJarUtil(
            config_jars['gcs_bucket'],
            builds_path = config_jars['builds_path'],
            nightly_path = config_jars['nightly_path'],
            jar_name_template = jars.DEFAULT_JAR_NAME_TEMPLATE,
            transport = self.transport)


    @staticmethod
    def format_jar_uri(bucket, jar):
        return 'gs://{}/{}'.format(bucket, jar)


    @classmethod
    def from_args(
            cls,
            user,
            job_name,
            args,
            stallion_config,
            transport=None,
            default_mode=JarModes.LOCALLY_BUILT):

        if getattr(args, JarModes.LATEST_PRODUCTION_BUILD.value.lstrip('-'), None):
            return LatestProductionBuildJar(
                    user,
                    job_name,
                    args.with_extra_jars,
                    stallion_config,
                    transport=transport)

        elif getattr(args, JarModes.LATEST_NIGHTLY.value.lstrip('-'), None):
            return LatestProductionNightlyJar(
                    user,
                    job_name,
                    args.with_extra_jars,
                    stallion_config,
                    transport=transport)

        elif getattr(args, JarModes.LATEST_SPARK_BUILD.value.lstrip('-'), None):
            return LatestSparkProductionBuildJar(
                    args.with_latest_spark_build,
                    user,
                    job_name,
                    args.with_extra_jars,
                    stallion_config,
                    transport=transport)

        elif getattr(args, JarModes.DATED_NIGHTLY.value.lstrip('-'), None):
            return ProductionNightlyJarFromDate(
                    args.with_nightly_jar_from_date,
                    user,
                    job_name,
                    args.with_extra_jars,
                    stallion_config,
                    transport=transport)

        elif getattr(args, JarModes.FIXED_URI.value.lstrip('-'), None):
            return FixedUriJar(
                args.with_fixed_jar_uri,
                user,
                job_name,
                args.with_extra_jars,
                stallion_config,
                transport=transport)

        elif getattr(args, JarModes.LATEST_FROM_BRANCH.value.lstrip('-'), None):
            return LatestJarFromBranch(
                args.with_latest_jar_from_branch,
                user,
                job_name,
                args.with_extra_jars,
                stallion_config,
                transport=transport)

        elif getattr(args, JarModes.LOCALLY_BUILT.value.lstrip('-'), None):
            return LocallyBuiltJar(
                args.use_cached_jars,
                args.nobuild,
                user,
                job_name,
                args.with_extra_jars,
                stallion_config,
                transport=transport)

        elif getattr(args, JarModes.LOCALLY_BUILT_SPARK.value.lstrip('-'), None):
            return LocallyBuiltSparklyJar(
                args.with_locally_built_spark_jar,
                args.use_cached_jars,
                args.nobuild,
                user,
                job_name,
                args.with_extra_jars,
                stallion_config,
                transport=transport)

        if not default_mode:
            raise Exception('No jar source setting was configured, and no '
                'default setting is provided!  Please specify one of '
                'the --with_XXX_jar settings')

        if default_mode not in JarModes:
            raise Exception('Invalid default_mode: {}.  Must be a member of '
                'the enumerated class JarModes: {}'.format(
                    default_mode,
                    ', '.join(JarModes)))

        # Make sure that default mode is one of the modes that do not require
        # additional parameters
        if default_mode not in ALLOWED_DEFAULT_JAR_MODES:
            raise Exception('No jar source setting was configured, and the default setting ({}) is invalid! Valid modes are [{}]'.format(
                default_mode,
                ', '.join(mode.value for mode in JarModes)))

        # Easiest way to enforce the default mode is to add it to the args
        # and make a recursive call to this same function, for reevaluation.
        print('No jar specification found: defaulting to {}'.format(
            default_mode.value))

        new_args = Namespace(**args.__dict__)
        setattr(new_args, default_mode.value.lstrip('-'), True)
        return cls.from_args(
                user,
                job_name,
                new_args,
                stallion_config,
                transport=transport,
                default_mode=None) # to prevent possible repeat calls in case of errors in the code above


    @staticmethod
    def check_gcs_uri(uri):
        split_gcs = namedtuple('SplitGcs', ['bucket', 'object', 'fragment'])
        split = uritools.urisplit(uri)

        if split.scheme != 'gs' or not split.authority or not split.path:
            raise ValueError('`{}` is not a valid GCS URI!'.format(uri))

        (jarname, ext) = os.path.splitext(os.path.basename(split.path))

        if ext != '.jar':
            raise ValueError('`{}` is not a JAR file!'.format(uri))

        return split_gcs(
                split.authority,
                split.path.lstrip('/'),
                split.fragment)


class LatestProductionBuildJar(JarManager):
    def get_primary_jar(self):
        util = self.get_jar_util()
        return self.format_jar_uri(
                util.builds_bucket,
                util.get_latest_build_jar())


class LatestProductionNightlyJar(JarManager):
    def get_primary_jar(self):
        util = self.get_jar_util()
        return self.format_jar_uri(
                util.nightly_bucket,
                util.get_latest_nightly_jar())


class LatestSparkProductionBuildJar(JarManager):
    def __init__(
            self,
            module,
            user,
            job_name,
            extra_jars,
            stallion_config,
            transport = None):
        super(LatestSparkProductionBuildJar, self).__init__(
                user,
                job_name,
                extra_jars,
                stallion_config,
                transport)

        self.module = module


    def get_primary_jar(self):
        util = self.get_jar_util()
        return self.format_jar_uri(
                util.builds_bucket,
                util.get_latest_spark_jar(self.module))


class ProductionNightlyJarFromDate(JarManager):
    def __init__(
            self,
            date,
            user,
            job_name,
            extra_jars,
            stallion_config,
            transport = None):
        super(ProductionNightlyJarFromDate, self).__init__(
                user,
                job_name,
                extra_jars,
                stallion_config,
                transport)

        self.date = date


    def get_primary_jar(self):
        util = self.get_jar_util()
        return self.format_jar_uri(
                util.nightly_bucket,
                util.get_nightly_jar(self.date))


class LatestJarFromBranch(JarManager):
    def __init__(
            self,
            branch,
            user,
            job_name,
            extra_jars,
            stallion_config,
            transport = None):
        super(LatestJarFromBranch, self).__init__(
                user,
                job_name,
                extra_jars,
                stallion_config,
                transport)

        self.branch = branch


    def get_primary_jar(self):
        util = self.get_jar_util()
        return self.format_jar_uri(
                util.dev_builds_bucket,
                util.get_latest_build_jar_for_branch(self.branch))


class FixedUriJar(JarManager):
    def __init__(self, uri, user, job_name, extra_jars, stallion_config, transport=None):
        super(FixedUriJar, self).__init__(
                user,
                job_name,
                extra_jars,
                stallion_config,
                transport)

        self.check_gcs_uri(uri)
        self.uri = uri

    def get_primary_jar(self):
        split = uritools.urisplit(self.uri)

        if not self.transport.exists(split.authority, split.path.lstrip('/')):
            raise jars.JarNotFoundException(self.uri)

        return self.uri


class LocallyBuiltJar(JarManager):
    JAR_PREFIX_PATH = 'jars'


    @property
    def local_jar_path(self):
        return '../../cloud-data-all-jobs/target/scala-2.11/{jar_name}'.format(
                jar_name=self.jar_name)


    @property
    def jar_name(self):
        return 'cloud-data-all-jobs-assembly-0.0.4-SNAPSHOT.jar'


    @property
    def bucket_name(self):
        return self.stallion_config['adhoc_jar_bucket']


    @property
    def gcs_blob_name(self):
        return os.path.join(
            self.JAR_PREFIX_PATH,
            self._get_gcs_jar_name(),
            self.jar_name)

    @property
    def gcs_jar_uri(self):
        return self.format_jar_uri(self.bucket_name, self.gcs_blob_name)


    def get_primary_jar(self):
        if self.use_cached_jars and self.transport.exists(
                self.bucket_name,
                self.gcs_blob_name):
            print('Using previously-uploaded JAR {}'.format(self.gcs_jar_uri))

        elif self.already_built:
            print('Using already-built and uploaded jar at {path}...'.format(path=self.gcs_jar_uri))

        elif not self.nobuild or not self._local_jar_exists():
            print('Rebuilding jar locally...')
            self._build_local_jar()
            print('Successfully rebuilt jar at path {path}'.format(path=self.local_jar_path))
            self._upload_local_jar()

            self.already_built = True

        return self.gcs_jar_uri

    def __init__(self, use_cached_jars, nobuild, user, job_name, with_extra_jars, stallion_config, transport=None):
        super(LocallyBuiltJar, self).__init__(
                user,
                job_name,
                with_extra_jars,
                stallion_config,
                transport)

        self.use_cached_jars = use_cached_jars
        self.nobuild = nobuild
        self.already_built = False


    def _local_jar_exists(self):
        return os.path.exists(self.local_jar_path)


    def _get_gcs_jar_name(self):
        clean_git_hash = None if not self.use_cached_jars else self.get_git_state()
        return self.job_name if not clean_git_hash else \
            '{user}-adhoc-{git_hash}'.format(user=self.user, git_hash=clean_git_hash)


    def _build_local_jar(self):
        subprocess.check_call(['../../script/sbt-gcp', 'cloud-data-all-jobs/assembly'])


    def _upload_local_jar(self):
        print("Uploading jars to GCP at %s" % os.path.dirname(self.gcs_jar_uri))
        self.transport.upload(self.bucket_name, self.gcs_blob_name, self.local_jar_path)

        return self.gcs_jar_uri


    @staticmethod
    def get_git_state(jar_repo_path=None):
        uncommitted_files = [
            _f for _f in \
            subprocess.check_output(['git', 'status', '--porcelain'], encoding='utf-8') \
                .strip().split('\n') if _f]
        if uncommitted_files:
            print('WARNING: unclean git state.  It is recommended to commit all files to assist in jar-caching and workflow-labeling')
            return None

        git_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD'], encoding='utf-8').strip()
        return git_hash


class LocallyBuiltSparklyJar(LocallyBuiltJar):
    JAR_PREFIX_PATH = 'jars/spark'


    def __init__(self, module, use_cached_jars, nobuild, user, job_name,
                 with_extra_jars, stallion_config, transport=None):
        super(LocallyBuiltSparklyJar, self).__init__(
                use_cached_jars,
                nobuild,
                user,
                job_name,
                with_extra_jars,
                stallion_config,
                transport)

        if not os.path.isdir(module):
            raise Exception('Could not find module {}. '
                'Please make sure the module name is correct and that you '
                'ran this command from the root of the sparkly repository.'.format(module))

        self.module = module


    @property
    def jar_name(self):
        return os.path.basename(self.local_jar_path)


    @property
    def local_jar_dir(self):
        return './{}/target/scala-2.12'.format(self.module)


    @property
    def local_jar_path(self):
        util = self.get_jar_util()
        latest_jar_name =  util.get_latest_local_spark_jar(self.local_jar_dir, self.module)

        return '{}/{}'.format(self.local_jar_dir, latest_jar_name)


    def _local_jar_exists(self):
        util = self.get_jar_util()
        return (os.path.isdir(self.local_jar_dir) and
                util.get_latest_local_spark_jar(self.local_jar_dir, self.module) is not None)


    def _build_local_jar(self):
        sbt_path = os.path.expanduser('~/software/sparkly-sbt')
        if not os.path.isfile(sbt_path):
            raise Exception('Could not find Sparkly sbt script! Please install it by '
                            'running \'scripts/get-sbt.sh\' from Sparkly\'s root directory')

        subprocess.check_call([sbt_path, 'assembly'])
