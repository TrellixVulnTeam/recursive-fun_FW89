from __future__ import print_function
from future import standard_library

#########
# futurize can add a call to `standard_library.install_aliases() and that call will fail on
# DEV vms, because those run python 2.7.5. and the ipv6 detection in parts of 2.7.5 have a bug
#
# This should be needed very rarely
import socket
original_has_ipv6 = socket.has_ipv6

from sys import version_info
if (version_info[0] == 2 and version_info[1] == 7 and version_info[2] == 5):
    socket.has_ipv6 = False

standard_library.install_aliases()
socket.has_ipv6 = original_has_ipv6
##########

from builtins import input
from builtins import str
from builtins import object
import abc
import argparse
from grpc import StatusCode
import json
import os
import re
import signal
import subprocess
import sys
from time import sleep
from typing import List
import pkg_resources
from zipfile import ZipFile
from uritools import urisplit
from tempfile import TemporaryFile

from time import time
from urllib.parse import urlparse

from etsy_dataproc_common_config import EtsyDataprocClusterConfig, WorkflowType, script_utils, jars
from etsy_dataproc_common_config.pysparkclient.uri_generator import URIGenerator
from etsy_dataproc_common_config.pysparkclient.job_config_generator import PysparkJobConfigGenerator
from etsy_dataproc_common_config.pysparkclient.create_cluster import add_pyspark_init_action, configure_pyspark_parser_args, configure_local_init_actions, add_default_hadoop_properties, add_default_custom_image
from etsy_dataproc_common_config.util import execute_partial_function_and_catch_reauth
import etsy_dataproc_common_config.util as common_config_util
from etsy_dataproc_common_config.adhoc_config import AdhocConfigLoader
from etsy_dataproc_common_config.jars import GcsTransport

from workflow_templates import dataproc, util
from workflow_templates.dataproc import RetriableGCPAPIException
from workflow_templates.workflow import JavaAction, ScaldingAction, SparkAction, PysparkAction, Template
from workflow_templates.jar_manager import JarManager, JarModes
from functools import partial
from future.utils import with_metaclass


class AdhocRunner(with_metaclass(abc.ABCMeta, object)):
    DEFAULT_BUCKET_ARG = '--default-bucket'
    CANONICAL_BUCKET_ARG = '--canonical-bucket'
    GATEWAY_HOST_ARG = '--gateway-host'
    REFERENCE_DATA_BUCKET_ARG = '--reference-data-bucket'
    LOGS_BUCKET_ARG = '--logs-bucket'
    READ_ONLY_BUCKET_ARG = '--read-only-bucket'
    SPARK_ENVIRONMENT_ARG = '--spark-environment-settings'

    POLL_INTERVAL = 3

    WORKFLOW_TYPE = WorkflowType.ADHOC

    DEFAULT_MAIN_CLASS_PACKAGE = "com.etsy.scalding.jobs"

    DEFAULT_JAR_MODE = JarModes.LOCALLY_BUILT
    ALLOWED_JAR_MODES = None

    # Consider these randomly intialized variables; any reasoning to change them will do.
    MAX_RETRIES = 5
    OVERBOOKED_RETRY_INTERVAL = 10
    QUOTA_REACHED_RETRY_INTERVAL = 30

    @classmethod
    def _get_base_parser(cls, default_project_id):
        parser = argparse.ArgumentParser(
            description=__doc__,
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog='[JOB_ARGS...]',
            conflict_handler='resolve'
        )
        script_utils.configure_parser(parser, project_id = default_project_id)
        JarManager.configure_parser(parser, cls.ALLOWED_JAR_MODES)

        parser.add_argument(
            '--run_async', action='store_true', default=False,
            help='Optional. Do not block on job finishing; return when cluster is successfully instantiated'),

        parser.add_argument(
                '--dev', action='store_true', default=False,
                help='Optional.  Set to true to allow scripts to run using development versions of workflow-templates')

        parser.add_argument(
                '--use_existing_cluster', action='store_true', default=False,
                help='Set to true to use an existing dataproc cluster.  Note that this requires that the --cluster_name and --region arguments be set to the target cluster')

        parser.add_argument(
            '--default-bucket', default=None,
            help='Optional. Specify your own default bucket for the job to read/write.')

        parser.add_argument(
            '--read-only-bucket', default=None,
            help='**BETA** Optional. Specify a bucket to read production data from in a read only way.')

        return parser

    @classmethod
    def get_parser(cls, default_project_id):
        parser = cls._get_base_parser(default_project_id)

        parser.add_argument(
            'job_class', metavar='JOB', help='Class of the job you want to run.')
        return parser

    @classmethod
    def get_driver(cls, script_args):
        return script_args.job_class

    @classmethod
    def parse_args(cls, default_project_id, args=None):
        """ Build a parser and parse a list of command-line args.  If args
            is provided, we parse it; otherwise the default argparser
            behavior is to parse sys.argv.

            :param default_project_id: default project id to use if --project_id
                is not provided as one of the command-line args
            :type default_project_id: string
            :param args: list of arguments, most commonly provided only in unit tests
            :type args: list<string>
        """
        parser = cls.get_parser(default_project_id)

        # we must explicitly handle passing the command-line args to
        # the parser, because we have to post-examine the parsed arguments
        # in order to satisfy the criterion "all args after the job class are
        # job arguments."  Note that parse_known_args respects the more standard
        # use of -- to delimit script args vs. job args, so we have the option
        # of using these scripts that way, too.
        args = args or sys.argv[1:]
        script_args, positional = parser.parse_known_args(args=args)

        # look up the first occurrence of the job class in the args list; it's
        # perfectly possible that the class occurs more than once, but the
        # parser would have treated any later occurrences as unknown arguments,
        # not as driver
        driver_position = args.index(cls.get_driver(script_args))

        # Check for a delimiter.  If one is present, then we know that the
        # behavior of the parser was nice and clean, so we can return with little
        # additional processing
        delimiter_position = None if '--' not in args else args.index('--')
        if delimiter_position:
            # There is a weird edge case in the behavior of the argparser,
            # in which the job class is permitted to be the first argument
            # after the delimiter.  So we slice the array at the larger of the
            # delimiter position and the job class position, rather than
            # strictly at the delimiter position.
            return (script_args, args[1+max(delimiter_position, driver_position):])

        # When no delimiter was used, there is an edge case to deal with:
        # the parser would have interpreted any job args that are also valid
        # script args, as script args.  So we have to go back and re-process
        # the script args, chopping off any args that follow the job class
        # so that they may not be misinterpreted as script args.

        # Note: all this craziness is covered by the unit tests.

        reparsed_script_args = parser.parse_args(args[:1+driver_position])
        return (reparsed_script_args, args[1+driver_position:])


    @property
    def default_data_bucket(self):
        return self.script_args.default_bucket or self.stallion_config['dataproc']['default_data_bucket']

    @property
    def canonical_data_bucket(self):
        return self.stallion_config['dataproc']['canonical_data_bucket']

    @property
    def gateway_host(self):
        return self.stallion_config['gateway_host']

    @property
    def reference_data_bucket(self):
        return self.stallion_config.get('reference_data_bucket')

    @property
    def logs_bucket(self):
        return self.stallion_config.get('dataproc',{}).get('logs_bucket')

    @property
    def read_only_bucket(self):
        return self.script_args.read_only_bucket

    # extra_args is an overrideable function that returns a list of extra
    # args that will be appended to the job args by the augmented_args()
    # method. The default AdhocRunner does not specify any extra args.
    def get_extra_args(self) -> List[str]:
        return []

    @property
    def augmented_args(self):
        return self.job_args + self.get_extra_args()


    @property
    def main_class(self):
        if not self.DEFAULT_MAIN_CLASS_PACKAGE or \
                self.driver.startswith(self.DEFAULT_MAIN_CLASS_PACKAGE):
            return self.driver

        return self.DEFAULT_MAIN_CLASS_PACKAGE + '.' + self.driver


    def _cleanup(self, *args):
        try:
            if self.operation_id:
                print("Canceling your job...")
                dataproc.cancel_operation(self.operation_id)
        except Exception:
            pass
        finally:
            print('Cancelled job submission.')
            sys.exit(1)

    @abc.abstractmethod
    def _get_actions(self):
        pass

    @abc.abstractmethod
    def _needs_onprem_hadoop(self):
        pass

    def update_args_for_cluster(self):
        pass

    def _get_cluster(self):
        user_id = self.user_email.split('@')[0]
        workflow_name = '{}-{}'.format(user_id, int(time()))

        # set use_vpn to True if _needs_onprem_hadoop() is True, otherwise
        # leave it set to None to use the script_args/stallion defaults
        if self._needs_onprem_hadoop():
            self.script_args.use_vpn = True

        self.update_args_for_cluster()

        return script_utils.cluster_config_from_args(
                args=self.script_args,
                stallion_config=self.stallion_config['dataproc'],
                workflow_type=WorkflowType.ADHOC,
                workflow_name = workflow_name,
                contact_email = self.user_email)


    def _get_labels(self):
        labels = {
                'adhoc_runner': self.__class__.__name__,
                'workflow_templates_version': pkg_resources.get_distribution('workflow_templates').version,
                }

        # Must sanitize the labels to ensure GCP won't reject them
        return { key: common_config_util.sanitize_label(value) for (key, value) in list(labels.items()) }

    def _get_cluster_placement(self):
        if not self.script_args.use_existing_cluster:
            return {
                    'managedCluster': self._get_cluster().create_placement(for_workflow_template=True)
                    }

        # Use existing cluster.  Check to make sure that the cluster name and
        # region are available.
        cluster_name = self.script_args.cluster_name
        region = self.script_args.region or self.stallion_config['dataproc']['region']

        if not cluster_name or not region:
            raise Exception(
                    'Cannot place workflow template on an existing cluster '
                    'because one or both of (cluster_name, region) arguments '
                    'are not available (found: {}, {}).  Please provide the '
                    'necessary arguments and try again.'.format(
                        cluster_name,
                        region))

        return {
                'clusterSelector': {
                    'clusterLabels': {
                        'goog-dataproc-cluster-name': cluster_name,
                        'goog-dataproc-location': region,
                        }
                    }
                }

    def _get_template_data(self):
        labels = self._get_labels()

        template = Template(
            workflow_id = self.name,
            labels = labels,
            cluster_placement = self._get_cluster_placement(),
            actions = self._get_actions())

        return template.data()

    def _is_valid_job_class(self):
        is_valid = True
        if self.jar_manager is not None:
            jar_uri = self.jar_manager.get_primary_jar()
            uri_split = urisplit(jar_uri)
            bucket = uri_split.authority
            jar_name = uri_split.path.lstrip('/')

            try:
                # download the jar from GCS and check if it has the job class
                f = TemporaryFile(mode='w+b', suffix='.jar')
                self.gcs.download(bucket, jar_name, f)
                jar_zip = ZipFile(f)
                job_path = self.main_class.replace('.', '/') + '.class'

                is_valid = job_path in jar_zip.namelist()
                f.close()
            except Exception as e:
                print("Failed to check for valid job class with Exception {}".format(e))
                print("Skipping check...")

        return is_valid


    def _ensure_dir(self):
        """
        Makes a best effort to find a CloudData directory and execute from there
        """

        working_dir = os.getcwd()
        exec_dir = os.path.abspath(sys.argv[0])
        working_big_data = re.search(r'/BigData(/|$)', working_dir)
        exec_big_data = re.search(r'/BigData(/|$)', exec_dir)
        working_cloud_data = os.path.join(working_dir[:working_dir.find('BigData')] + 'BigData', 'gcp', 'CloudData') if working_big_data else ""
        exec_cloud_data = os.path.join(exec_dir[:exec_dir.find('BigData')] + 'BigData', 'gcp', 'CloudData') if exec_big_data else ""
        default_cloud_data = os.path.expanduser('~/development/BigData/gcp/CloudData/')
        in_cloud_data = os.path.split(working_dir)[-1] == 'CloudData'

        if not (in_cloud_data or os.path.exists(working_cloud_data) or os.path.exists(exec_cloud_data) or os.path.exists(default_cloud_data)):
            raise Exception(
                'Could not find {}. If you have BigData checked out somewhere else, please'
                'go to the gcp/CloudData directory there, and try again'.format(default_cloud_data))

        if not in_cloud_data:
            for cloud_data in [working_cloud_data, exec_cloud_data, default_cloud_data]:
                if os.path.exists(cloud_data):
                    os.chdir(cloud_data)
                    break


    def run(self):
        signal.signal(signal.SIGINT, self._cleanup)

        if not isinstance(self, AdhocSparkRunner):
            self._ensure_dir()

        print('Checking if job class exists in the specified jar...')
        if not self._is_valid_job_class():
            raise Exception(
                'The job class {job_class} does not exist in the jar file located at {jar_uri}. '.format(
                    job_class=self.script_args.job_class,
                    jar_uri=self.jar_manager.get_primary_jar()) + (
                    'Please check the class definition again. ' +
                    'Perhaps a spelling mistake or the functions being referenced do not exist?'))

        workflow_data = self._get_template_data()

        print(json.dumps(workflow_data, indent=4))

        print('Submitting adhoc workflow to Google DataProc...')

        successfully_built_cluster = False
        retry_count = 0
        while not successfully_built_cluster:
            try:
                self.operation_id = dataproc.start_workflow_inline(self.project_id, self.region, workflow_data)
                print('Workflow submitted to Google Dataproc with operation ID {id}'.format(id=self.operation_id))

                # If we are told to use an existing cluster, do not poll and just set the cluster_name var
                if self.script_args.use_existing_cluster:
                    cluster_name = self.script_args.cluster_name
                else:
                    cluster_name = dataproc.poll_for_workflow_cluster_name(self.operation_id)

                successfully_built_cluster = True
            except RetriableGCPAPIException as e:
                if retry_count < self.MAX_RETRIES:
                    retry_count += 1
                else:
                    print('Max retry limit reached.')
                    raise e
                # For resource quota issues, no need to try different zone and also should wait longer before retry
                if e.code == StatusCode.ABORTED:
                    print('Create cluster failed. GCP error: {}. Retrying in {} seconds.'.format(e.message, self.QUOTA_REACHED_RETRY_INTERVAL))
                    sleep(self.QUOTA_REACHED_RETRY_INTERVAL)
                elif e.code == StatusCode.UNAVAILABLE:
                    print('Create cluster failed. GCP error: {}. Retrying in {} seconds.'.format(e.message, self.OVERBOOKED_RETRY_INTERVAL))
                    sleep(self.OVERBOOKED_RETRY_INTERVAL)
                else:
                    # If you've classified a new code of errors as retriable, please specify how to retry in this block of code
                    raise e

        print('\nCluster {cluster_name} instantiated for your workflow template.\n'.format(cluster_name=cluster_name))

        if self.run_asynchronously:
            print('Watch running jobs here: {jobs_url}. It may take a few seconds for your job to appear.'.format(
                jobs_url=dataproc.get_jobs_url(self.project_id)))
            sys.exit(0)

        print('Starting job poll for project {}, operation {}, region {}'.format(
            self.project_id,
            self.operation_id,
            self.region))
        dataproc.poll_for_workflow_job_completion(self.project_id, self.operation_id, self.region)

        print('All jobs complete.')


    @classmethod
    def from_args(
            cls,
            args=None,
            stallion_config=None,
            check_workflow_templates_version=False,
            jar_transport=None):
        """ Construct an AdhocRunner class from command-line args, and from an
            optional stallion_config (the latter is most useful for unit testing).
            :param args: Optional.  arguments to parse.
            :type args: list<string>
            :param stallion_config: Optional. stallion config to use
            :type stallion_config: dict
            :param check_workflow_templates_version: whether to require that the
                latest version of workflow-templates is installed
            :type check_workflow_templates_version: bool
            :param jar_transport: Transport object to use for looking up jars
                in GCS
            :type jar_transport: etsy_dataproc_common_config.jars.GcsTransport
        """

        print('Retrieving user credentials...')
        # Get user email from gcloud login, because it's not (necessarily?)
        # guaranteed that their local username will match their gcloud login
        (default_project_id, user_email) = script_utils.get_project_and_user()

        (script_args, job_args) = \
                cls.parse_args(default_project_id, args=args)

        return cls(
                user_email,
                script_args,
                cls.get_driver(script_args),
                job_args,
                stallion_config,
                check_workflow_templates_version,
                jar_transport)


    def get_jar_manager(
            self,
            user,
            job_name,
            args,
            stallion_config,
            transport,
            default_mode):

        return JarManager.from_args(
                user=user,
                job_name=job_name,
                args=args,
                stallion_config=stallion_config,
                transport=transport,
                default_mode=default_mode)

    def __init__(
            self,
            user_email,
            script_args,
            driver,
            job_args,
            stallion_config,
            check_workflow_templates_version,
            gcs_transport = None,
            jar_transport = None):
        self.project_id = script_args.project_id
        if not self.project_id:
            raise Exception("""
        No project_id found, either in the command-line args or in your gcloud defaults.
        Please either provide a --project_id argument, or set a default project via:

        gcloud config set project <my-project-id>

        """)

        self.user_email = user_email
        self.script_args = script_args
        self.driver = driver
        self.job_args = job_args

        self.run_asynchronously = script_args.run_async

        self.stallion_config = (
            stallion_config or
            execute_partial_function_and_catch_reauth(
                partial(AdhocConfigLoader, self.project_id)))
        self.region = script_args.region or self.stallion_config['dataproc']['region']

        self.user = self.user_email.split('@')[0]
        self.name = '{user}-{timestamp}'.format(user=self.user, timestamp=int(time()))

        self.gcs = gcs_transport or GcsTransport(self.project_id)
        self.jar_manager = self.get_jar_manager(
                self.user,
                self.name,
                script_args,
                self.stallion_config,
                jar_transport or self.gcs,
                self.DEFAULT_JAR_MODE)

        self.operation_id = None

        if check_workflow_templates_version:
            util.check_versions(script_args.dev)


class AdhocScaldingRunner(AdhocRunner):
    def get_extra_args(self) -> List[str]:
        extra_args = [
                self.CANONICAL_BUCKET_ARG, self.canonical_data_bucket,
                self.DEFAULT_BUCKET_ARG, self.default_data_bucket,
                self.GATEWAY_HOST_ARG, self.gateway_host,
                '--env', 'GCP',
                '--hdfs',
                ]

        if self.reference_data_bucket:
            extra_args += [self.REFERENCE_DATA_BUCKET_ARG, self.reference_data_bucket]

        if self.logs_bucket:
            extra_args += [self.LOGS_BUCKET_ARG, self.logs_bucket]

        if self.read_only_bucket:
            extra_args += [self.READ_ONLY_BUCKET_ARG, self.read_only_bucket]

        return extra_args

    def _get_actions(self):
        return [ScaldingAction(
            main_class=self.main_class,
            args=self.augmented_args,
            jars=self.jar_manager.jars)]

    def _needs_onprem_hadoop(self):
        return False


class AdhocJavaRunner(AdhocRunner):
    DEFAULT_MAIN_CLASS_PACKAGE = None

    def _get_actions(self):
        return [JavaAction(
            main_class=self.main_class,
            args=self.augmented_args,
            jars=self.jar_manager.jars)]

    def _needs_onprem_hadoop(self):
        return self.main_class in [
                JavaAction.GATEWAY_SINK_CLASS,
                JavaAction.DATACOPIER_CLASS ]


class AdhocMissingSourceDataCopier(AdhocJavaRunner):
    def get_extra_args(self) -> List[str]:
        extra_args = [
            self.CANONICAL_BUCKET_ARG, self.canonical_data_bucket,
            self.DEFAULT_BUCKET_ARG, self.default_data_bucket,
            self.GATEWAY_HOST_ARG, self.gateway_host,
            '--env', 'GCP',
            '--hdfs',
        ]

        if self.reference_data_bucket:
            extra_args += [self.REFERENCE_DATA_BUCKET_ARG, self.reference_data_bucket]

        if self.logs_bucket:
            extra_args += [self.LOGS_BUCKET_ARG, self.logs_bucket]

        return extra_args

    def get_missing_sources(self):
        def exists(source):
            split_source = JarManager.check_gcs_uri(source)

            # Thanks Google. We specify some folders as sources without the trailing slash,
            # but then GCS doesn't recognize them
            return self.gcs.exists(split_source.bucket, split_source.object) or \
                    self.gcs.exists(split_source.bucket, split_source.object + '/')

        env = os.environ
        env['HADOOP_ROOT_LOGGER'] = "ERROR,console"
        result = subprocess.check_output(
            ['hadoop', 'jar', AdhocRunner.LOCAL_JAR_PATH, 'cascading.cloud.Main',
             'com.etsy.scalding.runnable.BuildSourcesRunnable', self.main_class, '--only-flow-planner'] + self.args, env=env).strip()

        return [source for source in result.split("\n") if not exists(source)]

    def _get_actions(self):
        return [JavaAction(
                main_class=JavaAction.DATACOPIER_CLASS,
                args=['--run-on-source', 'false',
                      '--yarn-queue', 'default',
                      '--source', 'husky',
                      '--dest', 'GCP',
                      '--dirs', ','.join(self.copy_urls),
                      '--dest-bucket', self.default_data_bucket],
                jars=self.jar_manager.jars)]

    def _needs_onprem_hadoop(self):
        return True


    def get_copy_urls(self, missing_sources):
        non_canonical_sources = [source for source in missing_sources
                                 if not source.startswith('gs://' + self.canonical_data_bucket)]
        canonical_sources = [source for source in missing_sources
                                     if source.startswith('gs://' + self.canonical_data_bucket)]

        print('You are missing the following sources:')
        print('\n'.join(non_canonical_sources))

        if canonical_sources:
            print('\nWARNING: This script WILL NOT copy over the following missing canonical sources:\n{}\n'
                .format('\n'.join(canonical_sources)))
            print('Hop in #gcp-data-platform to ask about copying those over. Meanwhile...\n')

        opt = input('Copy over existing non-canonical sources from Husky? [y/n]\n').lower()
        while opt not in ['y', 'n']:
            opt = input('Please input y or n.\n').lower()

        if opt == 'n':
            sys.exit(1)

        return [urlparse(source).path for source in non_canonical_sources]


    def run(self):
        assert self.default_data_bucket != self.canonical_data_bucket, \
                'Canonical data cannot be copied to GCS!'

        print('Checking sources for {main_class} with args {args}'.format(
            main_class=self.main_class,
            args=self.augmented_args))
        missing_sources = self.get_missing_sources()
        if not missing_sources:
            print('All sources are present.')
            sys.exit(1)

        self.copy_urls = self.get_copy_urls(missing_sources)

        super(AdhocMissingSourceDataCopier, self).run()


class AdhocDataCopier(AdhocJavaRunner):
    """ A class for starting data-copier operations.  This class uses a
        different command-line pattern from the other adhoc runner classes,
        because it only supports a single job class (the DataCopier class)
        and set of args specific to that class.  To implement this, we
        override the parser methods in the parent class.
    """

    DEFAULT_JAR_MODE = JarModes.LATEST_PRODUCTION_BUILD

    @classmethod
    def get_parser(cls, default_project_id):
        parser = AdhocRunner._get_base_parser(default_project_id)

        datacopier_group = parser.add_argument_group('DataCopier-specific args')

        datacopier_group.add_argument(
                '--source',
                help='Source cluster for the data copier',
                required=True)

        datacopier_group.add_argument(
                '--dest',
                help='Destination cluster for the data copy',
                required=True)

        datacopier_group.add_argument(
                '--dirs',
                help='Comma-separated list of directories to copy.  Optionally, list entries may use the pattern <src>:<dest> in order to alter the destination path of the DataCopy operation',
                default=None)

        datacopier_group.add_argument(
                '--config',
                help='ADVANCED OPTION: A file listing the dirs to copy. Must be added to the jar you build as well',
                default=None
        )

        datacopier_group.add_argument(
                '--source-bucket',
                help='Bucket to copy from.  Only valid when --source=GCP',
                default=None)

        datacopier_group.add_argument(
                '--dest-bucket',
                help='Bucket to copy to.  Only valid when --dest=GCP',
                default=None)

        return parser


    @classmethod
    def parse_args(cls, default_project_id, args=None):
        """ Build a parser and parse a list of command-line args.  If args
            is provided, we parse it; otherwise the default argparser
            behavior is to parse sys.argv.

            :param default_project_id: default project id to use if --project_id
                is not provided as one of the command-line args
            :type default_project_id: string
            :param args: list of arguments, most commonly provided only in unit tests
            :type args: list<string>
        """
        parser = cls.get_parser(default_project_id)

        # we must explicitly handle passing the command-line args to
        # the parser, because we have to post-examine the parsed arguments
        # in order to satisfy the criterion "all args after the job class are
        # job arguments."  Note that parse_known_args respects the more standard
        # use of -- to delimit script args vs. job args, so we have the option
        # of using these scripts that way, too.
        args = args or sys.argv[1:]
        parsed_args = parser.parse_args(args=args)

        if parsed_args.source == parsed_args.dest == 'GCP':
            raise Exception('Please do not use this script for GCP-to-GCP data '
                'copy operations.  gsutil is much more efficient for these '
                'operations.')

        if parsed_args.source != 'GCP' and parsed_args.dest != 'GCP':
            raise Exception('Please do not use this script for HDFS-to-HDFS '
                'data copy operations.  the data_copier script in '
                'BigData/script is much more efficient for these operations.')

        if parsed_args.dirs and parsed_args.config:
            raise Exception('Must use exactly one of --dirs and --config')

        if not (parsed_args.dirs or parsed_args.config):
            raise Exception('Must use exactly one of --dirs and --config')

        datacopier_args = [
                '--source', parsed_args.source,
                '--dest', parsed_args.dest,
                '--run-on-source', str(parsed_args.source == 'GCP').lower(),
                '--yarn-queue', 'default',
                ]

        if parsed_args.dirs:
            datacopier_args += ['--dirs', parsed_args.dirs]
        else:
            datacopier_args += ['--config', parsed_args.config]

        if parsed_args.source_bucket:
            if parsed_args.source != 'GCP':
                raise Exception('Please do not specify --source-bucket when the source is not GCP!')

            datacopier_args += [ '--source-bucket', parsed_args.source_bucket ]

        if parsed_args.dest_bucket:
            if parsed_args.dest != 'GCP':
                raise Exception('Please do not specify --dest-bucket when the dest is not GCP!')

            datacopier_args += [ '--dest-bucket', parsed_args.dest_bucket ]

        parsed_args.job_class = JavaAction.DATACOPIER_CLASS

        return (parsed_args, datacopier_args)


    def _needs_onprem_hadoop(self):
        return True


class AdhocSparkRunner(AdhocRunner):
    DEFAULT_MAIN_CLASS_PACKAGE = None
    DEFAULT_JAR_MODE = None
    ALLOWED_JAR_MODES = [
        JarModes.FIXED_URI,
        JarModes.LATEST_SPARK_BUILD,
        JarModes.LOCALLY_BUILT_SPARK]

    def _get_actions(self):
        return [SparkAction(
            main_class=self.main_class,
            args=self.augmented_args,
            jars=self.jar_manager.jars)]

    def get_extra_args(self) -> List[str]:
        if self.stallion_config.get('spark_environment_settings'):
            spark_env_json = json.dumps(self.stallion_config.get('spark_environment_settings'))
            args = [self.SPARK_ENVIRONMENT_ARG, spark_env_json]
        else:
            args = []

        return args

    def _needs_onprem_hadoop(self):
        return False

class AdhocPysparkRunner(AdhocSparkRunner):

    def _get_actions(self):
        uri_generator = URIGenerator(
                self.script_args.repo,
                self.script_args.package_name,
                self.script_args.branch,
                self.script_args.major_version,
                self.script_args.patch_version,
                self.script_args.bucket_name,
                self.script_args.staging_bucket_name)

        job_config_generator = PysparkJobConfigGenerator(
                self.driver,
                self.job_args,
                uri_generator,
                self.script_args.py_files,
                self.script_args.jars,
                self.script_args.files,
                self.script_args.archives,
                self.script_args.ignore_default_archive,
                self.script_args.spark_properties,
                self.script_args.use_local_driver,
                self.script_args.local_archives)
        return [PysparkAction(job_config_generator)]


    def update_args_for_cluster(self):
        self.script_args = add_pyspark_init_action(self.script_args)
        self.script_args.hadoop_properties = add_default_hadoop_properties(self.script_args.hadoop_properties)
        self.script_args.dataproc_image_uri = add_default_custom_image(self.project_id)

    @classmethod
    def add_job_args(cls, parser):
        group = parser.add_argument_group('Arguments to configure the dataproc pyspark job to be submitted to a cluster')

        parser.add_argument(
                'driver_file',
                type=str,
                help='Driver script pythonfile name. The user can specify a full GCS URI (for the outstanding path to the driver file) or just the filename (which will retrieve the latest versioned driver script based on other params [like major, patch versions].). Alternatively, the user can provide a local path to a driver script and the `--local_driver_file` flag which will upload the local file to a staging folder in GCS to be used in the job.')

        parser.add_argument(
                '--use_local_driver',
                action='store_true',
                help='Flag to signify if driver_file specified is a local file')

        group.add_argument(
                '--archives',
                nargs="*",
                default=[],
                help='Optional. Whitespace separated list of archives to provide to job (only .zip, .tar, .tar.gz. or .tgz files). Can be either in GCS URI format (gs://) or be a local file that will be uploaded to GCS staging. Can also specify an alias for this package to be used by spark like `gs://sample_bucket/sample_path/sample_package.tar.gz#sample_env`. However, the alias, `environment` is reserved for the versioned package `python_libs.tar.gz` on GCS when --ignore_default_archive is not specified.')

        group.add_argument(
                '--local_archives',
                nargs="*",
                default=[],
                help='Optional. Whitespace separated list of local archives to provide to job (only .zip, .tar, .tar.gz. or .tgz files). These local files are uploaded to a staging folder on GCS to be used by the job. Can also specify an alias for this package to be used by spark like `./local_sample_path/sample_package.tar.gz#sample_env`. However, the alias, `environment` is reserved for the versioned package `python_libs.tar.gz` on GCS when --ignore_default_archive is not specified.')

        group.add_argument(
                '--ignore_default_archive',
                action='store_true',
                help='Optional. If true, then the job will not use the versioned default archive on GCS and will only use specified archives by the archives flag')

        group.add_argument(
                '--files',
                nargs="*",
                default=[],
                help='Optional. Whitespace separated list of files to be provided to the job.')

        group.add_argument(
                '--jars',
                nargs="*",
                default=[],
                help='Optional. Whitespace separated list of jar files to be provided to the executor and driver classpaths.')

        group.add_argument(
                '--spark_properties',
                type=json.loads,
                default={},
                help='Optional. Key value pairs to configure pyspark job. Available properties: https://spark.apache.org/docs/latest/configuration.html#available-properties. Example: "{\'spark.app.name\': \'sample-app\', \'spark.driver.memory\': \'1g\'}"')

        group.add_argument(
                '--py_files',
                nargs="*",
                default=[],
                help='Optional. Whitespace separated list of Python files to be provided to the job. Accepts following formats: .py, .zip, or .egg')

        group.add_argument(
                '--repo',
                default='pyspark_ml',
                help='Optional. Repo to retrieve archive artifact from. Defaulted to `pyspark_ml`')

        group.add_argument(
                '--package_name',
                help='Optional. Package to retrieve archive artifact from.')

        group.add_argument(
                '--branch',
                default='master',
                help='Optional. Branch to retrieve archive artifact from. Defaulted to `master`')

        group.add_argument(
                '--major_version', type=str,
                default=None,
                help='Optional. Major version of package to retrieve archive artifact from. Only available for master branch and by default uses the latest major version.')

        group.add_argument(
                '--patch_version', type=str,
                default=None,
                help='Optional. Patch version of package to retrieve archive artifact from. Only available for master branch and by default uses the latest patch version for a given major version.')

        group.add_argument(
                '--bucket_name',
                # default value set in pysparkclient.URIGenerator
                default=None,
                help='Optional. GCS bucket for git tags that separate artifact name from the version.')

        group.add_argument(
            '--dataproc_image_version', default='1.4-debian9',
            help='Optional. Dataproc image version. Defaulted to `1.4-debian9`')

        group.add_argument(
            '--num_preemptible_workers', type=int, default=0,
            help='Optional. Number of preemptible worker nodes. Defaulted to 0')

        group.add_argument(
            '--staging_bucket_name',
            default='pyspark-build-staging-87bo',
            help='Optional. GCS bucket for staging local pyspark artifacts to GCS for use by pyspark jobs.')

        return parser

    @classmethod
    def get_driver(cls, script_args):
        return script_args.driver_file

    @classmethod
    def get_parser(cls, default_project_id):
        parser = cls._get_base_parser(default_project_id)
        parser = configure_local_init_actions(parser)
        parser = configure_pyspark_parser_args(parser)
        parser = cls.add_job_args(parser)

        return parser

    def get_jar_manager(*args):
        return None
