from __future__ import print_function
from builtins import next
import googleapiclient.discovery
from grpc import StatusCode
import json

from oauth2client.client import GoogleCredentials
from time import sleep


API_NAME = 'dataproc'
API_VERSION = 'v1beta2'

CLIENT = None

JOBS_URL = 'https://console.cloud.google.com/dataproc/jobs'

POLL_INTERVAL = 3

COMPLETION_STATES = ['COMPLETED', 'FAILED']


def _raise_failure_exceptions(state):

    """
    Checks for both general workflow failure and cluster create failure, because
    the REST API is inconsistent.

    :param state: Operation (workflow) state returned by the Google REST API.
    """

    if 'error' in state:
        raise Exception('Workflow failed. Full state: {}'.format(
            json.dumps(state, indent=4)))
    # NOTE: are we ever going to get to these codepaths?
    # It looks like any error in the createCluster step is going to result in an error at the top level?
    # See: https://developers.google.com/resources/api-libraries/documentation/dataproc/v1beta2/python/latest/dataproc_v1beta2.projects.regions.operations.html#get
    elif 'createCluster' in state['metadata'] and 'error' in state['metadata']['createCluster']:
        raise Exception('Cluster creation step failed. Full state: {}'.format(
            json.dumps(state['metadata']['createCluster'], indent=4)))
    else:
        graph = state['metadata']['graph']
        if next((job for job in graph['nodes'] if job['state'] == 'FAILED'), None):
            raise Exception('One or more jobs failed. Full state: {}'.format(
                json.dumps(graph, indent=4)))

# NOTE will potentially merge this and above method once I get more context on what the above aims to do: see NOTE ^
def _raise_retriable_cluster_creation_exceptions(state):
    """
    Looks for a couple error cases at createCluster step that indicate we want to retry

    :param state Operation (workflow) state returned by the Google REST API.
    """

    if 'error' in state and 'code' in state['error'] and 'createCluster' in state['metadata']:
        code = state['error']['code']
        # https://grpc.io/grpc/python/grpc.html#grpc.StatusCode.UNAVAILABLE
        # Python ^ for comparison. The Go code below for more robust commenting on how google uses the StatusCode.
        # https://github.com/googleapis/googleapis/blob/master/google/rpc/code.proto#L180
        if code == StatusCode.UNAVAILABLE.value[0]:
            if 'message' in state['error']:
                message = state['error']['message']
            else:
                message = "GCP returned an unvailable error code for the cluster creation step."
            # tack on the error from the create cluster metadata, if available.
            if 'error' in state['metadata']['createCluster']:
                message = '{} {}'.format(message, state['metadata']['createCluster']['error'])
            raise RetriableGCPAPIException(message, StatusCode.UNAVAILABLE)
        # It's a little less clear cut what else the API will ABORTED for:
        # https://grpc.io/grpc/python/grpc.html#grpc.StatusCode.ABORTED
        # https://github.com/googleapis/googleapis/blob/master/google/rpc/code.proto#L138
        # So we also do a basic substring check to confirm the message does in fact refer to resource quotas.
        # NOTE: should GCP publish a strong guarantee of the shape of these metadata objects,
        # we might want to give them the first-class object treatment rather than this optimistic dictionary hunting
        elif (code == StatusCode.ABORTED.value[0] and
             'error' in state['metadata']['createCluster'] and
             'quota' in state['metadata']['createCluster']['error']):
             raise RetriableGCPAPIException(state['metadata']['createCluster']['error'], StatusCode.ABORTED)


def _get_parent(project_id, region):
    return 'projects/{project}/regions/{region}'.format(project=project_id, region=region)


def cancel_operation(operation_id, thread_safe=True):
    return get_client(thread_safe).projects().regions().operations().cancel(
        name=operation_id).execute()


def get_operation_state(operation_id, thread_safe=True):
    return get_client(thread_safe).projects().regions().operations().get(
        name=operation_id).execute()


def delete_workflow(workflow_id, thread_safe=True):
    return get_client(thread_safe).projects().regions().workflowTemplates().delete(
        name=workflow_id).execute()


def start_workflow(workflow_id, thread_safe=True):
    response = get_client(thread_safe).projects().regions().workflowTemplates().instantiate(
        name=workflow_id, body={}).execute()

    return response['name']


def start_workflow_inline(project_id, region, workflow_data, thread_safe=True):
    response = get_client(thread_safe).projects().regions().workflowTemplates().instantiateInline(
        parent=_get_parent(project_id, region), body=workflow_data).execute()

    return response['name']


def upload_workflow(project_id, region, workflow_data, thread_safe=True):
    response = get_client(thread_safe).projects().regions().workflowTemplates().create(
        parent=_get_parent(project_id, region), body=workflow_data).execute()

    return response['name']


def get_jobs_url(project_id):
    return '{jobs_url}?project={project_id}'.format(
        jobs_url=JOBS_URL, project_id=project_id)


def get_job_url(project_id, job_id, region):
    return '{jobs_url}/{job_id}?project={project}&region={region}'.format(
        jobs_url=JOBS_URL, job_id=job_id, project=project_id, region=region)


def poll_for_workflow_cluster_name(operation_id, thread_safe=True):

    """
    Polls operation state via REST API until the full unique cluster name is populated in metadata.
    Required because a UUID is appended to the given cluster name when it is instantiated.

    :param operation_id: String
    :return: cluster_name: String
    """

    while True:
        state = get_operation_state(operation_id, thread_safe)
        # Only adding this here for now, as the cases we're interested in regard cluster creation.
        _raise_retriable_cluster_creation_exceptions(state)
        _raise_failure_exceptions(state)

        if 'clusterName' in state['metadata']:
            return state['metadata']['clusterName']
        else:
            sleep(POLL_INTERVAL)


def poll_for_workflow_job_completion(project_id, operation_id, region, thread_safe=True):

    """
    Polls operation state via REST API to announce each running job in the workflow
    by printing a link to its logs.
    Once a job has started and been announced, its metadata will not be reprinted.
    Continues to poll until the workflow is finished or fails.

    :param operation_id: String
    :param region: String
    :return: True if workflow completes successfully, Exception raised otherwise
    """

    running_or_completed_jobs = []

    while True:
        state = get_operation_state(operation_id, thread_safe)
        _raise_failure_exceptions(state)

        jobs = state['metadata']['graph']['nodes']
        incomplete_jobs = [step for step in jobs if step['state'] not in COMPLETION_STATES]

        if len(incomplete_jobs) == 0:
            return True
        else:
            new_running_jobs = [job for job in jobs
                                if job['stepId'] not in running_or_completed_jobs
                                and job['state'] == 'RUNNING']
            for job in new_running_jobs:
                print('Step {step_id} is now running on DataProc.\n\tYou can track its progress at {job_url}\n'.format(
                    step_id=job['stepId'], job_url=get_job_url(project_id, job['jobId'], region)))

            running_or_completed_jobs = running_or_completed_jobs + [job['stepId'] for job in new_running_jobs]

            sleep(POLL_INTERVAL)

def get_workflow_state(operation_id, thread_safe=True):
    """
    Retrieves the state of a workflow via REST API by its operation_id. Returns that state.
    :param operation_id: String
    """
    state = get_operation_state(operation_id, thread_safe)
    try:
        _raise_failure_exceptions(state)
    except Exception as e:
        return 'FAILED'

    jobs = state['metadata']['graph']['nodes']
    incomplete_jobs = [step for step in jobs if step['state'] not in COMPLETION_STATES]

    if len(incomplete_jobs) == 0:
        return 'COMPLETED'

    return 'IN_PROGRESS'

def get_client(thread_safe):
    # httplib2 (a dep of the google-api) is not thread safe -- when we call this,
    # give the caller the option to create a new client, should they need thread safety
    if thread_safe:
        return googleapiclient.discovery.build(
            API_NAME,
            API_VERSION,
            credentials=GoogleCredentials.get_application_default(),
            cache_discovery=False)
    else:
        return get_singleton_client()

def get_singleton_client():
    """
    If the client is None, instantiate and return it. Otherwise, return the global var.
    Encapsulate this in a function so that our unit tests stop failing.
    """
    global CLIENT
    if CLIENT is None:
        CLIENT = googleapiclient.discovery.build(
            API_NAME,
            API_VERSION,
            credentials=GoogleCredentials.get_application_default(),
            cache_discovery=False
        )
    return CLIENT

class RetriableGCPAPIException(Exception):
    def __init__(self, message, code):
        self.message = message
        self.code = code
