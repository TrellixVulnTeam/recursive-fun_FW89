import argparse

from time import time
from etsy_dataproc_common_config import script_utils
from etsy_dataproc_common_config.util import execute_partial_function_and_catch_reauth
from etsy_dataproc_common_config.adhoc_config import AdhocConfigLoader
from etsy_dataproc_common_config.jars import GcsTransport
from functools import partial
from workflow_templates import util
from workflow_templates.jar_manager import JarManager, JarModes


class AdhocSparkGetter(object):

    ALLOWED_JAR_MODES = [
        JarModes.LOCALLY_BUILT_SPARK,
        JarModes.LATEST_SPARK_BUILD,
    ]
    DEFAULT_JAR_MODE = None

    @classmethod
    def from_args(
            cls,
            args=None,
            stallion_config=None,
            check_workflow_templates_version=False,
            jar_transport=None):
        """ Construct an AdhocSparkArtifactGetter class from command-line args, and from an
                optional stallion_config (the latter is most useful for unit testing).
            :param args: Optional.  arguments to parse.
            :type args: list<string>
            :param stallion_config: Optional. stallion config to use
            :type stallion_config: dict
            :param check_workflow_templates_version: whether to require that the
                latest version of workflow-templates is installed
            :type check_workflow_templates_version: bool
            :param jar_transport: Transport object to use for looking up jars in GCS
            :type jar_transport: etsy_dataproc_common_config.jars.GcsTransport
        """

        print('Retrieving user credentials...')
        (default_project_id, user_email) = script_utils.get_project_and_user()

        script_args = cls.parse_args(default_project_id, args=args)

        return cls(
            user_email,
            script_args,
            stallion_config,
            check_workflow_templates_version,
            jar_transport)

    @classmethod
    def parse_args(cls, default_project_id, args=None):
        parser = argparse.ArgumentParser(
            formatter_class=argparse.RawTextHelpFormatter,
            description=(
                "Returns Spark items like artifacts, configs, etc.. Example Usage: \n"
                "gcp_get_spark artifact --with_latest_spark_build common\n"
                "gcp_get_spark artifact --nobuild --with_locally_built_spark_jar jobs\n"
                "gcp_get_spark artifact --with_locally_built_spark_jar example-jobs\n"
            ))

        parser.add_argument(
            '--project_id',
            default=default_project_id,
            help='GCP project denoting which stallion config will be used. '
                'Can also be set via `gcloud config set project <my-project-id>`')

        parser.add_argument(
            '--dev',
            action='store_true',
            default=False,
            help='Optional. Set to true to allow scripts to run using development '
                'versions of workflow-templates')

        # Subparser: artifact (i.e., gcp_get_spark artifact)
        subparsers = parser.add_subparsers(
            help='Possible spark values to get')

        artifact_subparser = subparsers.add_parser(
            'artifact',
            help='Retrieves artifact information from specified jar.')

        JarManager.configure_parser(
            artifact_subparser,
            cls.ALLOWED_JAR_MODES,
            add_extra_jars_arg=False)

        # Subparser: config (i.e., gcp_get_spark config)
        config_parser = subparsers.add_parser(
            'config',
            help='Stub. Not yet implemented.')

        return parser.parse_args(args=args)

    def __init__(
            self,
            user_email,
            script_args,
            stallion_config,
            check_workflow_templates_version,
            jar_transport=None):

        self.project_id = script_args.project_id

        if not self.project_id:
            raise Exception(
                "No project_id found, either in the command-line args or in your gcloud defaults.\n"
                "Please either provide a --project_id argument, or set a default project via:\n\n"
                "gcloud config set project <my-project-id>")

        self.user_email = user_email

        self.script_args = script_args

        self.stallion_config = (
            stallion_config or
            execute_partial_function_and_catch_reauth(
                partial(AdhocConfigLoader, self.project_id)))

        self.user = self.user_email.split('@')[0]
        self.name = '{user}-{timestamp}'.format(user=self.user, timestamp=int(time()))

        self.gcs = GcsTransport(self.project_id)

        # NOTE: In parse_args, we specify the removal of an argument, --with_extra_jars
        # using JarManager.configure_parser that does does have any relevance to our usage
        # of it as a simple getter. Because the static method JarManager.from_args requires
        # it set, we set it again here before it is passed
        self.script_args.__dict__['with_extra_jars'] = []
        self.jar_manager = self.get_jar_manager(
            self.user,
            self.name,
            script_args,
            self.stallion_config,
            jar_transport or self.gcs,
            self.DEFAULT_JAR_MODE)

        if check_workflow_templates_version:
            util.check_versions(script_args.dev)

    def get_jar_manager(
            self,
            user,
            job_name,
            args,
            stallion_config,
            transport,
            default_mode):

        return JarManager.from_args(
            user=user,
            job_name=job_name,
            args=args,
            stallion_config=stallion_config,
            transport=transport,
            default_mode=default_mode)

    def run(self):
        jar_uri = self.jar_manager.get_primary_jar()
        print("Jar URI: {}\n".format(jar_uri))

        print(
            "Include the following in the first cell of your Jupyter notebook in order to "
            "import classes contained in the JAR:\n")
        print("```")
        print(
            '%%init_spark\n'
            'launcher.conf.set("spark.jars", "{}")\n'.format(jar_uri))
        print("```")
