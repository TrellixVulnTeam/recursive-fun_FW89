from builtins import object
import os

class JavaAction(object):
    DATACOPIER_CLASS = 'com.etsy.hdfs.DataCopier'
    GATEWAY_SINK_CLASS = 'com.etsy.db.GatewaySink'
    JOB_TYPE = 'hadoopJob'

    @staticmethod
    def step_id(main_class):
        return main_class.split('.')[-1]

    def __init__(self, main_class, args, jars, step_id=None, prerequisite_step_ids=None):
        self.main_class = main_class
        self.args = args
        self.jars = jars
        self.step_id = step_id or self.step_id(main_class)
        self.prerequisite_step_ids = prerequisite_step_ids


    def create_hadoop_job(self):
        job = {
            'stepId': self.step_id,
            self.JOB_TYPE: {
                'args': self.args,
                'jarFileUris': self.jars,
                'mainClass': self.main_class
            }
        }

        if self.prerequisite_step_ids:
            job['prerequisiteStepIds'] = self.prerequisite_step_ids

        return job


class ScaldingAction(JavaAction):
    MAIN_CLASS = 'cascading.cloud.Main'
    RUNNABLE_CLASS = 'com.etsy.scalding.ScaldingRunnable'

    def __init__(self, main_class, args, jars, step_id=None, prerequisite_step_ids=None):
        args = [self.RUNNABLE_CLASS, main_class] + args
        step_id = step_id or JavaAction.step_id(main_class)

        super(ScaldingAction, self).__init__(
            main_class=self.MAIN_CLASS,
            args=args,
            jars=jars,
            step_id=step_id,
            prerequisite_step_ids=prerequisite_step_ids)


class SparkAction(JavaAction):
    JOB_TYPE = 'sparkJob'


class PysparkAction(object):
    JOB_TYPE = 'pysparkJob'

    @staticmethod
    def step_id(driver_file):
        return os.path.basename(driver_file).split('.')[0]

    def __init__(self, job_config_generator, step_id=None):
        self.job_config_generator = job_config_generator
        self.step_id = step_id or self.step_id(self.job_config_generator.get_driver_uri())

    def create_hadoop_job(self):
        return {
                'stepId': self.step_id,
                self.JOB_TYPE: self.job_config_generator.create_job_config(),
                }

class Template(object):
    def data(self):
        return {
            'id': self.id,
            'labels': self.labels,
            'placement': self.cluster_placement,
            'jobs': self.jobs
        }

    def __init__(self, workflow_id, labels, cluster_placement, actions):
        self.id = workflow_id
        self.labels = labels
        self.cluster_placement = cluster_placement
        self.jobs = []

        for action in actions:
            if not isinstance(action, JavaAction) and not isinstance(action, PysparkAction):
                raise Exception('Only Java, Scalding, Pyspark actions supported at this time')

            self.jobs.append(action.create_hadoop_job())
