import abc
import itertools
import json
import os
import pprint
import logging
import yaml
import shutil
import tempfile
import traceback
import requests
import six
import marshmallow as ma
import uritools

from json.decoder import JSONDecodeError
from google.cloud import storage
from stallion.components.base import BaseCodeDeployComponent, BaseConfigPushHook
from stallion.retry import retryable
from stallion.schemas.base import PermissiveSchema
from stallion_etsy_plugin.shared_schemas import DataprocSchema, DatacopClientSchema, JarsSchema
from stallion_etsy_plugin.components.base import BaseServicePermissiveSchema
from stallion.deploy import git
from typing import Dict, List


class TooManyDagNodesException(Exception):
    pass


class AirflowRetriableTestFailureException(Exception):
    pass


class DagConfigSchema(PermissiveSchema):
    contact_email = ma.fields.String(missing='gcp-data-platform@etsy.com')
    dag_owner = ma.fields.String(missing = 'data-platform')


class AirflowSchema(BaseServicePermissiveSchema):
    dataproc = ma.fields.Nested(DataprocSchema, required=True)
    dags = ma.fields.Nested(DagConfigSchema, missing = DagConfigSchema().load({}).data)

    datacop = ma.fields.Nested(DatacopClientSchema, required=True)

    gateway_host = ma.fields.URL(required=True)
    gcp_copy_cluster = ma.fields.String(required=False)
    jars = ma.fields.Nested(JarsSchema, required=True)
    reference_data_bucket = ma.fields.String(required=False)
    dag_submit_batch_size = ma.fields.Integer(required=False)
    max_nodes_per_dag = ma.fields.Integer(required=False)

    spark_environment_settings = ma.fields.Dict(required=False)

class AirflowCommonPushHook(BaseConfigPushHook):
    ALLOWED_FILE_EXTENSIONS = frozenset([ '.yaml', '.yml' ])

    @abc.abstractproperty
    def INPUT_SUBDIRS():
        pass


    def input_paths(self, component):
        return [ os.path.join(component.config_dir, subdir) for subdir in self.INPUT_SUBDIRS ]


    def file_iter(self, component):
        for dags_dir in self.input_paths(component):
            if not os.path.isdir(dags_dir):
                logging.info('No DAGs found in path %s', dags_dir)
                continue

            dag_files = [ os.path.join(dags_dir, dag_file) for dag_file in os.listdir(dags_dir) ]

            for dag_file in dag_files:
                (_, ext) = os.path.splitext(dag_file)
                if ext not in self.ALLOWED_FILE_EXTENSIONS:
                    raise Exception('Unexpected file extension `{}` for file `{}` in dags dir {}'.format(
                        ext, dag_file, dags_dir))

                yield dag_file

    def get_loaded_dags(self, component):
        loaded_dags = []

        for filename in self.file_iter(component):
            (_, ext) = os.path.splitext(filename)
            if ext not in ['.yaml', '.yml']:
                logging.warn(
                    'Non-yaml DAG found: %s.  Cannot retrieve dag name!',
                    filename)
                continue

            loaded = None
            with open(filename) as _in:
                loaded = list(yaml.safe_load_all(_in))

            if not loaded:
                logging.warn('No content found for dag file %s', filename)
                continue

            loaded_dags.append(loaded)

        return loaded_dags

    def ensure_unique_cluster_names(self, component):
        cluster_names = set()
        dupes = []

        for loaded in self.get_loaded_dags(component):
            resources = loaded[0].get('resources')
            if not resources:
                continue

            for resource in resources:
                if resource.get('type') == 'dataproc_cluster':
                    cluster_name = resource.get('properties').get('cluster_name')
                    if cluster_name:
                        if '<<' in cluster_name and '>>' in cluster_name:
                            # Skip any cluster_name that has custom python code in it
                            continue
                        elif cluster_name in cluster_names:
                            dupes.append(cluster_name)
                        else:
                            cluster_names.add(cluster_name)

        if dupes:
            raise ValueError('Duplicate dataproc cluster names found: {}'.format(dupes))

    def ensure_unique_dag_names(self, component):
        """ Retrieves the complete list of DAG names stored in the configuration
            for the provided component.  Raises an exception if the names are not
            unique.

            :param component: an Airflow component
            :type component: Component

            :returns: a set of dag names
            :rtype: set<string>
        """
        dag_names = set()
        dupes = []

        for loaded in self.get_loaded_dags(component):
            dag_name = loaded[0].get('name')
            if not dag_name:
                continue

            if dag_name in dag_names:
                dupes.append(dag_name)

            dag_names.add(dag_name)

        if dupes:
            raise ValueError('Duplicate DAG names found: {}'.format(dupes))

        return dag_names

    def _get_dag_client(self, remote_uri):
        # import in here so that we don't need to include etsy_dag_client
        # in client-only installations of stallion
        from etsy_dag_client import Client

        # This is a temporary hack until all of the deploy processes have moved to cloudbuild
        # and use unauthenticated Airflow endpoints.  Creating a client on cloudbuild will return an exception
        # with 404 in the message because default credentials cannot be retrieved from the metadata server
        #
        # The exception code in etsy_dag_client is:
        #     raise Exception('Metadata server returned an invalid status code: {}'.format(resp.status_code))
        try:
            client = Client(host_uri=remote_uri)
        except Exception as e:
            if 'invalid status code: 404' in str(e):
                logging.info('Regular etsy_dag_client could not be created. Trying unauthenticated.')
                client = Client(host_uri=remote_uri, unauthenticated=True)
            else:
                raise e

        return client

    @staticmethod
    def _args_from_dict(data, exclude_keys):
        def kebab(key):
            return key.replace('_', '-')

        scalar_args = [ ['--{}'.format(kebab(key)), value] for (key, value) in six.iteritems(data)
                if key not in exclude_keys and not isinstance(value, (bool, list)) ]

        flags = [ ['--{}'.format(kebab(key))] for (key, value) in six.iteritems(data)
                if key not in exclude_keys and isinstance(value, bool) and value ]

        list_args = [ ['--{}'.format(kebab(key)) ] + value for (key, value) in six.iteritems(data)
                if key not in exclude_keys and isinstance(value, list) ]

        return list(map(str, itertools.chain.from_iterable(scalar_args + flags + list_args)))


class AirflowDagPushHook(AirflowCommonPushHook):
    INPUT_SUBDIRS = ['dags']

    @retryable((AirflowRetriableTestFailureException,), tries=2, delay=2, backoff=2, logger=logging)
    def run(self, component, deployer, config_manager, dry_run):
        if not isinstance(component, Airflow):
            raise Exception("Invalid component type: AirflowDagPushHook only works with `airflow`, not `{}`".format(component.type))

        remote_uri = deployer.get_deployment_uri(component)

        self.ensure_unique_dag_names(component)
        self.ensure_unique_cluster_names(component)
        dag_files = list(self.file_iter(component))

        max_nodes_per_dag = (
            component.config['max_nodes_per_dag']
            if 'max_nodes_per_dag' in component.config
            else None)
        if dry_run:
            # We should use boundary-layer to check the DAG files for errors,
            # even though we are not submitting them
            errors: List[str] = self._test_dag_files(remote_uri, dag_files, max_nodes_per_dag)
            if errors:
                raise AirflowRetriableTestFailureException(
                    'Errors building Airflow DAGs from boundary-layer configs: \n{}'.format(
                        '\n'.join('{}'.format(msg) for msg in errors)))

            logging.info('  dry run: skipping DAG sync')
            return

        batch_size = (
            component.config['dag_submit_batch_size']
            if 'dag_submit_batch_size' in component.config
            else None)
        if len(dag_files) > 0:
            self._sync_dags_with_remote(remote_uri, dag_files, batch_size)
        else:
            logging.info('No DAGs to submit')

        logging.info('Attempting to TTL test dags from test server...')
        test_dag_ids = self._get_test_dag_ids(remote_uri, ttl_days=4)
        if len(test_dag_ids) > 0:
            logging.info('Found the following test dags on server: {}'.format(test_dag_ids))
            results = self._delete_test_dag_ids(remote_uri, test_dag_ids)
            logging.info('TTL completed. Results: {}'.format(results))
        else:
            logging.info('Not a test airflow instance or no test DAGs to TTL. Continuing...')

    @retryable((Exception,), tries=4, delay=4, backoff=2, logger=logging)
    def _submit_dag_files(self, remote_uri, dag_files, batch_size=None):
        num_dags = len(dag_files)
        if not batch_size:
            batch_size = num_dags   # there will be only 1 batch in this case
        dags_batches = [
            dag_files[i:min(i + batch_size, num_dags)] for i in range(0, num_dags, batch_size)]

        logging.info('%d DAG files will be uploaded in %d batches.',
            num_dags,
            len(dags_batches))

        success_uploads = []
        fail_uploads = []
        for batch in dags_batches:
            logging.info('Pushing %d / %d DAG files in this batch to Airflow host at %s:\n    %s',
                len(batch),
                num_dags,
                remote_uri,
                '\n    '.join(batch))
            response = self._get_dag_client(remote_uri).submit_files(batch)

            if response.status_code != 200:
                raise Exception('Failure deploying DAGs to remote host {}: '
                        'Returned status {}, content {}'.format(
                            remote_uri,
                            response.status_code,
                            response.text))

            response_dict = response.json()
            logging.debug('DAG server returned:\n%s', pprint.pformat(response_dict))

            success_uploads += response_dict['success']
            fail_uploads += response_dict['error']

        logging.info('%d DAGs uploaded successfully', len(success_uploads))
        logging.info('%d DAG uploads failed', len(fail_uploads))

        if fail_uploads:
            raise Exception('Failure deploying DAGs: %s', fail_uploads)

        return [item['installer']['dag'] for item in success_uploads]

    @retryable((Exception,), tries=4, delay=4, backoff=2, logger=logging)
    def _get_remote_dags(self, remote_uri):
        logging.info(
                'Retrieving list of DAG files to Airflow host at %s',
                remote_uri,
                )
        response = self._get_dag_client(remote_uri).list_dags(include_disabled=False)
        if response.status_code != 200:
            raise Exception('Error retrieving dag list: {}'.format(response.content))

        return response.json()['items']

    def _sync_dags_with_remote(self, remote_uri, dag_files, batch_size):
        submitted_dags = self._submit_dag_files(remote_uri, dag_files, batch_size)
        submitted_dag_ids = frozenset(dag['dag_id'] for dag in submitted_dags)

        remote_dags = self._get_remote_dags(remote_uri)
        remote_dag_ids = frozenset(dag['dag_id'] for dag in remote_dags)

        to_delete = remote_dag_ids - submitted_dag_ids

        if not to_delete:
            logging.info('Remote DAG repository is up-to-date')
            return

        logging.info(
                'Deleting remote DAGs that are not present locally: %s',
                ','.join(to_delete))

        self._delete_dags(remote_uri, to_delete)

    @retryable((Exception,), tries=4, delay=4, backoff=2, logger=logging)
    def _delete_dags(self, remote_uri, to_delete):
        dag_client = self._get_dag_client(remote_uri)
        errors = {}
        for dag_id in to_delete:
            response = dag_client.disable_dag(dag_id, purge_airflow_db=True)

            error_string = None
            try:
                if response.status_code != 200 or response.json()['status'] != 'success':
                    error_string = response.content
            except JSONDecodeError:
                error_string = str(response)

            if error_string:
                errors[dag_id] = error_string
                continue

        if errors:
            raise Exception('Error deleting dag(s): {}'.format(errors))

    def _test_dag_files(self, remote_uri, dag_files, max_nodes_per_dag=None) -> List[str]:
        tmpdir = tempfile.mkdtemp()
        errors = []
        batch_size = 20
        num_dags = len(dag_files)
        client = self._get_dag_client(remote_uri)

        if max_nodes_per_dag is not None:
            for filename in dag_files:
                node_count = client.get_node_count(filename)
                if node_count > max_nodes_per_dag:
                    errors.append("Found {} nodes for dag {}. Max number of nodes per dag is {}".format(
                        node_count,
                        filename,
                        max_nodes_per_dag))

        # Files must be rendered individually, but then tested in batches
        rendered_files = [client._prepare_file(f, tmpdir) for f in dag_files]
        file_batches = [rendered_files[i:min(i + batch_size, num_dags)] for i in range(0, num_dags, batch_size)]

        for batch in file_batches:
            try:
                raw_response = client.test_files(batch)

                # The requests.Response object might be JSON, but it's not guaranteed to be. If it's not
                # then assume something went wrong and consider the entire response an error.
                error_string = None
                try:
                    response = raw_response.json()
                    if response['error']:
                        error_string = str(response['error'])
                except JSONDecodeError:
                    error_string = str(raw_response)

                if error_string:
                    errors.append(error_string)

            except Exception:
                errors.append(traceback.format_exc())

        shutil.rmtree(tmpdir)
        return errors

    def _get_test_dag_ids(self, remote_uri: str, ttl_days: int = 7) -> List[str]:
        """
        Calls an endpoint on Airflow where the etsy-test-api service is active.
        If the service does not exist (e.g., on production airflows), this call
        will return an empty list.
        """
        list_uri = uritools.urijoin(
            remote_uri,
            '/etsy-test-api/list_test_dags?ttl_days={}'.format(ttl_days))
        response = requests.get(list_uri, timeout=10)
        test_dag_ids = []
        if response.status_code == 200:
            try:
                payload = response.json()
                test_dag_ids = payload.get('test_dags') if 'test_dags' in payload else []
            except JSONDecodeError as _e:
                logging.info('Received JSONDecodeError on call to {}.'.format(list_uri))
                logging.info('Note: This is okay on Production airflow instances.')
        return test_dag_ids

    def _delete_test_dag_ids(self, remote_uri: str, test_dag_ids: List[str]) -> Dict[str, str]:
        """
        Calls an endpoint on Airflow where the etsy-test-api service is active.
        This call will delete test_dag_ids from the airflow DB.
        """
        delete_uri = uritools.urijoin(remote_uri, '/etsy-test-api/delete')
        response = requests.post(
            delete_uri,
            data=json.dumps({'test_dags': test_dag_ids}),
            headers={'Content-Type': 'application/json'},
            timeout=600)

        results = {}
        if response.status_code == 200:
            try:
                results = response.json()
            except JSONDecodeError as e:
                logging.info(
                    'Received JSONDecodeError error on call to {}. Full error: {}'.format(
                        delete_uri,
                        e))
        return results


class AirflowSQLPushHook(AirflowCommonPushHook):
    INPUT_SUBDIRS = ['dags']

    def run(self, component, deployer, config_manager, dry_run):
        if not isinstance(component, Airflow):
            raise Exception("Invalid component type: AirflowSQLPushHook only works with `airflow`, not `{}`".format(component.type))

        remote_uri = deployer.get_deployment_uri(component)

        dag_files = list(self.file_iter(component))

        jars_bucket = component.config['jars']['gcs_bucket']

        if dry_run:
            # TODO: Add a sql syntax checker

            return

        # TODO: Add a sql syntax checker
        self._sync_sql_with_remote(remote_uri, dag_files, jars_bucket)

    def _sync_sql_with_remote(self, remote_uri, dag_files, jars_bucket):
        # Parse yaml DAG files to get all sql versions specified'.
        submitted_sql_versions, submitted_sql_filenames = self._get_submitted_sql_configs(dag_files)

        if not submitted_sql_versions:
            logging.info('No SQL versions to sync. Skipping sql sync.')
            return None

        # Check sql plugin config is input syntax is correct,
        # and the sql versioned tar file exists in GCS location
        errors = self._verify_sql_config(submitted_sql_versions, submitted_sql_filenames, jars_bucket)
        if errors:
            raise Exception(errors)

        # Get latest SQL version from git tags
        latest_prod_version = None
        if 'latest' in submitted_sql_versions:
            latest_prod_version = str(git.get_latest_version(
                owner = 'Engineering',
                repository = 'BigData',
                tag_name = 'sql',
                tag_separator = '@'))

            if latest_prod_version not in submitted_sql_versions:
                submitted_sql_versions.append(latest_prod_version)

            submitted_sql_versions.remove('latest')

        self._post_sql_versions(remote_uri, submitted_sql_versions, latest_prod_version)

    def _get_submitted_sql_configs(self, dag_files):
        sql_versions = list()
        sql_filenames = list()
        for file_path in dag_files:
            with open(file_path) as _in:
                loaded = list(yaml.safe_load_all(_in))[0]

                try:
                    sql_versions.append(loaded['plugin_config']['etsy']['sql']['version'])
                except KeyError:
                    pass

                for operator in loaded.get('operators', []):
                    try:
                        sql_filenames.append(operator['properties']['sql'])
                    except KeyError:
                        pass

        return list(set(sql_versions)), list(set(sql_filenames))

    def _verify_sql_config(self, versions, filenames, jars_bucket):
        storage_client = storage.Client()
        bucket = storage_client.get_bucket(jars_bucket)
        errors = list()

        if not all(os.path.splitext(filename)[1] == '.sql' for filename in filenames):
            raise ValueError('Error, sql parameters must specify a sql filename appened'
                            'with ".sql": {}'.format(filenames))

        for version in versions:
            if version == 'latest':
                continue

            if version.split('-')[-1] == 'dev':
                version_path = 'sql/development/{}.tar'.format(version)
            else:
                version_path = 'sql/production/{}.tar'.format(version)

            if not storage.Blob(bucket=bucket, name=version_path).exists(storage_client):
                errors.append('Version path {} does not exist in GCS'
                              'bucket {}'.format(version_path, jars_bucket))

        return errors

    @retryable((Exception,), tries=5, delay=6, backoff=2, logger=logging)
    def _post_sql_versions(self, remote_uri, sql_versions, latest_version=None):
        logging.info('Pushing SQL versions to Airflow host at %s:\n    %s',
        remote_uri,
        '\n    '.join(sql_versions),
        )

        if 'latest' in sql_versions:
            sql_versions.remove('latest')

        response = self._get_dag_client(remote_uri).submit_sql_versions(sql_versions, latest_version)

        if response.status_code != 200:
            raise Exception('Failure deploying SQL to remote host {}: '
                    'Returned status {}, content {}'.format(
                        remote_uri,
                        response.status_code,
                        response.text))


class Airflow(BaseCodeDeployComponent):
    type = 'airflow'
    schema = AirflowSchema
    allowed_deployment_types = frozenset(['gae-flex', 'gce'])

    config_push_hooks = [ AirflowSQLPushHook(), AirflowDagPushHook()]

    deploy_files = ['proxy_config.yaml', 'env.list', 'etsy-airflow.conf']
