import datetime
from enum import Enum
from etsy_dataproc_common_config import util
import pkg_resources
import random
import uuid
import os

from past.builtins import basestring
from future.utils import iteritems

import googleapiclient.discovery
from oauth2client.client import GoogleCredentials
from googleapiclient.errors import HttpError


MASTER_MAX_ALLOWED_CORES = 64
WORKER_MAX_ALLOWED_CORES = 64
MIN_VERSION_REQUIRED_FOR_COMPONENT_GATEWAY = '1.3'
MAJOR_VERSION_ALLOWING_DATAPROC_INIT_ACTIONS_MODE = '1'


def _get_client():
    # REST documentation can be found here:
    # https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig
    API_NAME = 'dataproc'
    API_VERSION = 'v1beta2'

    CLIENT = googleapiclient.discovery.build(
        API_NAME,
        API_VERSION,
        credentials=GoogleCredentials.get_application_default(),
        cache_discovery=False
    )

    return CLIENT


def _get_resource_name(project_id, region, autoscaling_policy_id=None):
    if autoscaling_policy_id:
        return 'projects/{project}/regions/{region}/autoscalingPolicies/{policy_id}'.format(project=project_id, region=region, policy_id=autoscaling_policy_id)
    else:
        return 'projects/{project}/regions/{region}'.format(project=project_id, region=region)


class WorkflowType(Enum):
    ADHOC = 'adhoc'
    GATEWAY_SINK = 'sink'
    SCHEDULED = 'scheduled'
    SNAPSHOT = 'snapshot'

# constants defining allowed values for cluster lifecycle settings.  They are
# the same for both idle ttl and auto ttl.
# These values come from API responses to bad requests.  Not sure where else they
# may be documented...
MIN_TTL_SEC = 10 * 60
MAX_TTL_SEC = 336 * 3600


class EtsyDataprocClusterConfig(object):
    """
    Constructs the configuration blob used to create all Etsy Dataproc clusters.

    To use:
      1. Build the EtsyDataprocClusterConfig object
      2. Make modifications using the relevant set_XXX method
      3. Call conf.create_placement() to construct a configuration object
      4. Pass that object to the DataProc API.

    :param project_id: name of the GCP project in which to create the cluster
    :type project_id: str
    :param workflow_type: Type of workflow (adhoc / scheduled / sink / etc)
    :type workflow_type: WorkflowType (enum)
    :param workflow_name: Name of the workflow that this cluster is associated with
    :type workflow_name: str
    :param contact_email: Email address of the person/team that should be
        contacted for questions / error reports / etc
    :type contact_email: str
    :param cluster_name: Name of the cluster. If omitted, this will be derived
        from the workflow_type and workflow_name
    :type cluster_name: str
    :param owner: Name of the team or person who owns the cluster. If omitted,
        this will be derived from the contact_email parameter
    :param sahale_username: Username to report to the Sahale monitoring tool;
        if omitted, this will be derived from the owner parameter
    :type sahale_username: str
    :param region: GCP region in which to create the cluster.  Note that only
        us-central1 is presently supported, but this parameter is present for
        the future...
    :type region: str
    :param zone: GCP zone in which to create the cluster.  If omitted, the cluster
        config will allow DataProc to auto-select a zone within the cluster's
        region
    :type zone: str
    :param enable_http_port_access: If true, enable http access to specific
        ports on the cluster from external sources. Required to be True if you
        are using the component gateway. Defaults to false. Documentation here:
        https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#EndpointConfig
    :type enable_http_port_access: boolean
    :param optional_components: The set of optional components to activate on the cluster.
        Full list can be found here:
        https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#Component
    :type optional_components list
    :param internal_ip_only Optional. If true, all instances in the cluster will only have
        internal IP addresses. This is the Etsy default. If set to false, clusters will have
        ephemeral external IP addresses assigned to each instance. This internalIpOnly restriction
        can only be enabled for subnetwork enabled networks, and all off-cluster dependencies
        must be configured to be accessible without external IP addresses.
    :type internal_ip_only bool
    """

    # The cloud-platform access scope is set, which allow full access to all Cloud APIs
    DEFAULT_SERVICE_ACCOUNT_SCOPES = [
        "https://www.googleapis.com/auth/cloud-platform"
    ]
    def __init__(
            self,
            project_id,
            workflow_type,
            workflow_name,
            contact_email,
            cluster_name,

            stallion_config,

            owner = None,
            sahale_username = None,

            # The parameters below are all optional overrides for their
            # counterparts in the stallion config
            region=None,
            zone=None,
            logs_bucket=None,
            master_machine_type=None,
            master_disk_size_gb=None,
            num_workers=None,
            num_preemptible_workers=None,
            worker_machine_type=None,
            worker_disk_size_gb=None,
            autoscaling_policy=None,
            image_version=None,
            image_uri=None,
            labels=None,
            hadoop_properties=None,
            service_account=None,
            canonical_writer=False,
            use_vpn=None,
            network_tags=None,
            dns_init_script_uri=None,
            init_actions_uris=None,
            init_action_timeout_sec=None,
            metadata=None,
            subnetwork_uri=None,
            idle_delete_ttl_sec=None,
            auto_delete_ttl_sec=None,
            auto_delete_time=None,
            additional_service_account_scopes=None,
            enable_http_port_access=None,
            optional_components=None,
            see_all_notebooks=None,
            spark_application_history_location=None,
            spark_cluster_key_prefix=None,
            internal_ip_only=True,
            use_data_lake=None,
            ):

        self.project_id = project_id

        if workflow_type not in WorkflowType:
            raise ValueError('Unsupported workflow type {}'.format(workflow_type))

        self.workflow_type = workflow_type

        self.workflow_name = workflow_name

        if len(contact_email.split('@')) != 2 or \
                not contact_email.endswith('@etsy.com'):
            raise ValueError('Invalid contact_email: please include an @etsy.com address')

        self.contact_email = contact_email
        self.owner = owner or self.contact_email.split('@')[0]

        self.sahale_username = sahale_username or self.owner

        self.cluster_name = cluster_name or self.build_cluster_name(
                workflow_type,
                workflow_name)

        self.stallion_config = stallion_config

        # Make sure region and zone are valid
        self.region = region or stallion_config['region']
        util.validate_region(self.region)

        self.zone = zone or stallion_config['zone']
        util.validate_zone(self.zone)

        self.set_logs_bucket(logs_bucket or stallion_config.get('logs_bucket'))

        self.set_master_config(
                machine_type=master_machine_type or stallion_config['master_machine_type'],
                disk_size_gb=master_disk_size_gb or stallion_config['master_disk_size_gb'])

        # be careful to allow num_workers == 0 as a valid setting, i.e. check
        # to see whether num_workers is None vs. 0, both of which evaluate to
        # False in conditionals.
        self.set_worker_config(
                num_workers=num_workers if num_workers is not None else stallion_config['num_workers'],
                num_preemptible_workers=num_preemptible_workers if num_preemptible_workers is not None else stallion_config.get('num_preemptible_workers',0),
                machine_type=worker_machine_type or stallion_config['worker_machine_type'],
                disk_size_gb=worker_disk_size_gb or stallion_config['worker_disk_size_gb'])

        self.autoscaling_policy = autoscaling_policy

        image_version = image_version or stallion_config['image_version']
        if image_version:
            self.set_image_version(image_version)

        self.set_image_uri(image_uri)

        self.use_data_lake = use_data_lake if use_data_lake is not None else stallion_config.get("use_data_lake", False)

        self.set_ttl(
            idle_delete_ttl_sec=idle_delete_ttl_sec if idle_delete_ttl_sec is not None else stallion_config.get('idle_delete_ttl_sec'),
            auto_delete_ttl_sec=auto_delete_ttl_sec if auto_delete_ttl_sec is not None else stallion_config.get('auto_delete_ttl_sec'),
            auto_delete_time=auto_delete_time,
        )

        self.set_labels(labels)

        self.set_metadata(metadata)

        self.set_properties(hadoop_properties)

        self.set_init_actions(
            actions=init_actions_uris,
            timeout_sec=init_action_timeout_sec)

        if canonical_writer:
            if not stallion_config.get('canonical_service_account'):
                raise Exception(
                        'canonical-writer clusters are not supported '
                        'by this stallion config')

            if service_account is not None and service_account != \
                    stallion_config['canonical_service_account']:
                raise Exception(
                        'You may not specify a different service account '
                        '(found: {}) for canonical data-writer clusters ('
                        'stallion config specifies {})'.format(
                            service_account,
                            stallion_config['canonical_service_account']))

            self.set_service_account(stallion_config['canonical_service_account'])

        else:
            self.set_service_account(
                    service_account or stallion_config['default_service_account'])


        self.set_service_account_scopes(additional_service_account_scopes)

        use_vpn = use_vpn or stallion_config['use_vpc']

        if use_vpn:
            self.set_network_config(
                use_vpn=True,
                tags=network_tags if network_tags is not None else stallion_config.get('sharedvpc_network_tags', []),
                # On the Shared VPC, we will always use internal IPs only
                internal_ip_only = True,
                dns_init_script_gcs_uri=dns_init_script_uri or stallion_config.get('dns_init_script_uri'),
                subnetwork_uri = subnetwork_uri or stallion_config.get('sharedvpc_subnetwork_uri'),
            )

        else:
            self.set_network_config(
                use_vpn=False,
                tags=network_tags if network_tags is not None else stallion_config.get('default_network_tags', []),
                # On private subnets, if users are developing and they need
                # to ssh into the master node, we should allow them to set
                # external IPs and be able to ssh in. This should not be used
                # in scheduled Production jobs though
                internal_ip_only = internal_ip_only,
                dns_init_script_gcs_uri = None,
                subnetwork_uri = subnetwork_uri or stallion_config.get('default_subnetwork_uri'),
            )

        self.set_enable_http_port_access(enable_http_port_access)

        self.set_optional_components(optional_components)

        self.set_see_all_notebooks(see_all_notebooks)

        self.set_jupyter_notebook_bucket(stallion_config.get('jupyter_notebook_bucket'))

        self.spark_application_history_location = \
            self.determine_spark_application_history_location(spark_application_history_location,
                                                        stallion_config.get('spark_application_history_location'))

        self.set_spark_cluster_key_prefix(
            spark_cluster_key_prefix or stallion_config.get('spark_cluster_key_prefix'))

    @staticmethod
    def from_stallion_config(**kwargs):
        """ This is for backwards compatibility; we should use the new
            stallion-oriented constructor in basically all cases
        """
        return EtsyDataprocClusterConfig(**kwargs)


    @staticmethod
    def build_cluster_name(workflow_type, workflow_name):
        return '{}-{}'.format(workflow_type.value, workflow_name)


    def set_network_config(
            self,
            use_vpn,
            tags,
            internal_ip_only,
            dns_init_script_gcs_uri,
            subnetwork_uri):

        assert (use_vpn is False) or (dns_init_script_gcs_uri is not None), \
                'Must specify a dns_init_script_gcs_uri when using VPN!'

        assert (use_vpn is False) or (internal_ip_only is True), \
                'Must specify internal_ip_only when use_vpn is true!'

        if dns_init_script_gcs_uri:
            util.validate_gcs_uri(dns_init_script_gcs_uri)

        self.use_vpn = use_vpn

        base_tags = tags or []
        # The hadoop network tag is necessary for vpn-enabled clusters to allow
        # the workers and master to communicate
        self.network_tags = list(set(base_tags + ['hadoop'])) if use_vpn else base_tags

        self.internal_ip_only = internal_ip_only
        self.dns_init_script_gcs_uri = dns_init_script_gcs_uri

        if use_vpn and subnetwork_uri and subnetwork_uri != self._vpn_subnetwork_uri():
            raise Exception('Invalid subnetwork URI for the etsy shared vpc: '
                    '{} (only {} is allowed)'.format(
                        subnetwork_uri,
                        self._vpn_subnetwork_uri()))

        self.subnetwork_uri = self._vpn_subnetwork_uri() if use_vpn else (
                subnetwork_uri or self._private_subnetwork_uri())

        return self


    def set_master_config(self, disk_size_gb, machine_type):
        self.master_disk_size_gb = disk_size_gb

        util.validate_node_config(machine_type, MASTER_MAX_ALLOWED_CORES)
        self.master_machine_type = machine_type

        return self


    def set_worker_config(self, num_workers, num_preemptible_workers, disk_size_gb, machine_type):
        util.validate_node_config(machine_type, WORKER_MAX_ALLOWED_CORES)

        self.num_workers = num_workers
        self.worker_disk_size_gb = disk_size_gb
        self.num_preemptible_workers = num_preemptible_workers

        util.validate_worker_config(num_workers, num_preemptible_workers)
        self.worker_machine_type = machine_type

        return self


    def set_service_account(self, service_account = None):
        if not isinstance(service_account, (basestring, type(None))):
            raise TypeError(
                    'Invalid `service_account`: {} (expected a string)'.format(
                        service_account))

        self.service_account = service_account

        return self


    def set_image_uri(self, image_uri):
        if image_uri is not None and not isinstance(image_uri, basestring):
            raise TypeError(
                'Invalid `image_uri` argument: {} (expected a string)'.format(
                    image_uri))

        self.image_uri = image_uri

        return self


    def set_image_version(self, image_version):
        if not isinstance(image_version, basestring):
            raise TypeError(
                    'Invalid `image_version` argument: {} (expected a string)'.format(
                        image_version))

        self.image_version = image_version

        return self


    def set_labels(self, labels):
        self.labels = labels or {}

        if not isinstance(self.labels, dict):
            raise TypeError(
                    'Invalid `labels` argument: {} (expected a dictionary)'.format(
                        labels))

        return self

    def set_metadata(self, metadata):
        self.metadata = metadata or {}

        if self.use_data_lake:
            # Add data lake specific metadata. The order of this dict update matters.
            # If identical key values are specified by user,
            # then it should overwrite the default specified via stallion.
            data_lake_metadata = {
                "hive-metastore-instance": self.stallion_config["data_lake_metadata"]["hive_metastore_instance"],
                "metastore-proxy-port": str(self.stallion_config["data_lake_metadata"]["metastore_proxy_port"]),
                "use-cloud-sql-private-ip": str(self.stallion_config["data_lake_metadata"]["use_cloud_sql_private_ip"]).lower(),
                "kms-key-uri": self.stallion_config["data_lake_metadata"]["kms_key_uri"],
                "db-hive-user": self.stallion_config["data_lake_metadata"]["db_hive_user"],
                "db-hive-password-uri": self.stallion_config["data_lake_metadata"]["db_hive_password_uri"]
            }
            data_lake_metadata.update(self.metadata)

            self.metadata = data_lake_metadata

        if not isinstance(self.metadata, dict):
            raise TypeError(
                    'Invalid `metadata` argument: {} (expected a dictionary)'.format(
                        metadata))

        for key, value in iteritems(self.metadata):
            if not isinstance(key, basestring) or not isinstance(value, basestring):
                raise TypeError(
                    'Invalid `metadata` keys and values: {} (expected a string)'.format(
                        metadata))

        return self

    def set_service_account_scopes(self, additional_service_account_scopes):
        if additional_service_account_scopes:
            self.service_account_scopes = list(set(self.DEFAULT_SERVICE_ACCOUNT_SCOPES + \
                                                   additional_service_account_scopes))
        else:
            self.service_account_scopes = self.DEFAULT_SERVICE_ACCOUNT_SCOPES

        for scope in self.service_account_scopes:
            if not isinstance(scope, basestring):
                raise TypeError(
                    'Invalid `service_account_scopes` element: {} (expected a string)'.format(
                        scope))


    def set_logs_bucket(self, dataproc_logs_bucket = None):
        """ Set the bucket to which logs will be exported, for consumption
            by a YARN history server
        """
        if dataproc_logs_bucket is None:
            self.dataproc_logs_bucket = None
            return self

        self.dataproc_logs_bucket = dataproc_logs_bucket

        # Be forgiving if we were given a bucket including a leading gs://
        if self.dataproc_logs_bucket.startswith('gs://'):
            self.dataproc_logs_bucket = self.dataproc_logs_bucket[len('gs://'):]

        # but not if we were given a path
        if '/' in self.dataproc_logs_bucket:
            raise ValueError('Invalid argument `dataproc_logs_bucket`: {} (must be a bucket name, not a path or URI'.format(
                dataproc_logs_bucket))

        return self


    def set_spark_cluster_key_prefix(self, spark_cluster_key_prefix):
        if spark_cluster_key_prefix and spark_cluster_key_prefix.split('.')[0] != 'spark':
            raise ValueError('spark_cluster_key_prefix must start with \'spark\', i.e. spark.etsy.config')

        self.spark_cluster_key_prefix = spark_cluster_key_prefix


    def _build_properties(self):
        properties = {
                'core:sahale.custom.user.name': self.sahale_username,
                'core:fs.permissions.umask-mode': '000',
                'hdfs:fs.permissions.umask-mode': '000',
                'mapred:mapreduce.job.user.name': self.sahale_username,

                # Set a generous cascading counter timeout because of issues
                # we have seen in collecting counters for sahale
                'core:cascading.step.counter.timeout': '60',

                # disable logging to stackdriver because it costs a huge
                # amount of money, and the logs are not useful anyway (we use
                # our historyserver and log-proxy services instead)
                'dataproc:dataproc.logging.stackdriver.enable': 'false',

                # prevent users from letting the Spark history server delete files in GCS
                'spark:spark.history.fs.cleaner.enabled': 'false',
            }

        if self.num_workers == 0:
            # For single-node cluster info, see:
            # https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/single-node-clusters
            properties['dataproc:dataproc.allow.zero.workers'] = 'true'

        if self.dataproc_logs_bucket:
            properties.update({
                    'yarn:yarn.log-aggregation-enable': 'true',
                    'yarn:yarn.nodemanager.remote-app-log-dir': self._logs_bucket_path('/logs/hadoop'),
                    'yarn:yarn.log-aggregation.retain-seconds': '-1',
                })

        # Update hadoop_properties to include cluster settings as Spark config.
        # These may be invalid Spark settings which will be ignored, but with
        # this we can see cluster settings when viewing Spark jobs.
        self._add_cluster_config_to_hadoop_properties()

        # Do not allow the base properties defined so far to be overridden
        duplicate_keys = set(properties) & set(self.hadoop_properties)
        if duplicate_keys:
            raise ValueError(
                    'Invalid `properties` arg: {} (overwrites fixed keys {})'.format(
                        self.hadoop_properties,
                        duplicate_keys))

        # Now we define some user-adjustable properties.  These are job properties
        # that we have migrated over from BigData.  They can be overridden by the
        # values in self.hadoop_properties.  See:
        # https://github.etsycorp.com/Engineering/BigData/blob/master/operators/src/main/resources/etsy-job-conf-cluster.properties
        properties.update({
                'mapred:mapred.compress.map.output': 'true',
                'mapred:mapred.map.output.compression.codec': 'org.apache.hadoop.io.compress.SnappyCodec',
                'mapred:mapred.map.tasks': '400',
                'mapred:mapred.reduce.tasks': '200',
                # reduce the log move time from 180s to 10s so the internal Job History server can get the logs
                # consistently - The main entry point will block for 11s to ensure
                'mapred:mapreduce.jobhistory.move.interval-ms': '10000',
                # Set this globally because this parameter isn't always respected
                # at the job level and we frequently use counter names > the default
                # of 64. See MAPREDUCE-6925 for some details and history
                'yarn:mapreduce.job.counters.counter.name.max': '256',

                # Dataproc Spark History Server - limit the application loading to prevent master node memory and
                # CPU pressure
                'spark:spark.history.retainedApplications': '2',
                'spark:spark.history.ui.maxApplications': '10',

                # This is a temporary config included to prevent clusters from running jobs before
                # init scripts complete. REMOVE after below Google ticket is addressed.
                # https://enterprise.google.com/supportcenter/managecases#Case/0016000000LVy7C/U-19432140
                'dataproc:dataproc.worker.custom.init.actions.mode': 'RUN_BEFORE_SERVICES',

                # Add Lzo and Lzop compression codecs
                'core:io.compression.codecs': 'org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,'
                'org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,'
                'org.apache.hadoop.io.compress.Lz4Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec',
                'core:io.compression.codec.lzo.class': 'com.hadoop.compression.lzo.LzoCodec',
            })
        if self.worker_machine_type == 'e2-standard-8':
            properties.update({
                'spark:spark.dynamicAllocation.executorIdleTimeout': '1200',
                'yarn:yarn.nodemanager.resource.memory-mb': '30000',
                'yarn:yarn.nodemanager.resource.cpu-vcores': '8',
                'spark:spark.driver.memory': '2g',
                'spark:spark.driver.cores': '1',
                'spark:spark.executor.cores': '1',
                'spark:spark.executor.memory': '2g'
            })
        if self.worker_machine_type == 'n1-standard-8':
            properties.update({
                'spark:spark.dynamicAllocation.executorIdleTimeout': '1200',
                'yarn:yarn.nodemanager.resource.memory-mb': '29000',
                'yarn:yarn.nodemanager.resource.cpu-vcores': '8',
                'spark:spark.driver.memory': '2g',
                'spark:spark.driver.cores': '1',
                'spark:spark.executor.cores': '1',
                'spark:spark.executor.memory': '2g'
            })

        if self.image_uri is not None:
            # If we are given a dataproc image_uri, we expect additional jars to be in this path.
            # There does not seem to be any repercussions if this directory is empty but set it only
            # if we use a custom dataproc image to be safe
            properties.update({
                'spark:spark.driver.extraClassPath': '/usr/lib/dataproc-jars/*',
                'spark:spark.executor.extraClassPath': '/usr/lib/dataproc-jars/*',
            })

        properties.update(self.hadoop_properties)

        properties.update(self._get_jupyter_properties())

        if self.spark_application_history_location is not None:
            properties.update({'spark:spark.history.fs.logDirectory': self.spark_application_history_location})
            properties.update({'spark:spark.eventLog.dir': self.spark_application_history_location})

        # it can't hurt to validate again, in case we made a bad edit to one of the
        # hard-coded properties, above
        util.validate_property_keys(properties)

        return properties


    def set_properties(self, properties = None):
        self.hadoop_properties = properties or {}

        if self.use_data_lake:
            # Add data lake specific properties. The order of this dict update matters.
            # If identical key values are specified by user,
            # then it should overwrite the default specified via stallion.
            data_lake_properties = {
                "hive:hive.metastore.warehouse.dir": self.stallion_config["data_lake_properties"]["hive_metastore_warehouse_dir"],
                "spark:spark.sql.sources.partitionOverwriteMode": self.stallion_config["data_lake_properties"]["spark_sql_sources_partitionOverwriteMode"]
            }
            data_lake_properties.update(self.hadoop_properties)

            self.hadoop_properties = data_lake_properties

        util.validate_property_keys(self.hadoop_properties)

        return self


    def set_init_actions(self, actions = None, timeout_sec = None):
        self.init_actions = actions or []
        self.init_action_timeout_sec = timeout_sec

        util.validate_cluster_init_actions(self.init_actions)


    def set_ttl(
            self,
            idle_delete_ttl_sec=None,
            auto_delete_ttl_sec=None,
            auto_delete_time=None):
        self.idle_delete_ttl_sec=idle_delete_ttl_sec
        if idle_delete_ttl_sec is not None:
            if idle_delete_ttl_sec < MIN_TTL_SEC or idle_delete_ttl_sec > MAX_TTL_SEC:
                raise Exception(
                    'Invalid idle_delete_ttl_sec value `{}`: outside allowed range of ({}, {})'.format(
                        idle_delete_ttl_sec,
                        MIN_TTL_SEC,
                        MAX_TTL_SEC))


        if auto_delete_ttl_sec and auto_delete_time:
            raise Exception(
                    'Invalid cluster configuration: Cannot set both '
                    'auto_delete_ttl_sec and auto_delete_time!')
        self.auto_delete_ttl_sec=auto_delete_ttl_sec

        if auto_delete_ttl_sec is not None:
            if auto_delete_ttl_sec < MIN_TTL_SEC or auto_delete_ttl_sec > MAX_TTL_SEC:
                raise Exception(
                    'Invalid auto_delete_ttl_sec value `{}`: outside allowed range of ({}, {})'.format(
                        auto_delete_ttl_sec,
                        MIN_TTL_SEC,
                        MAX_TTL_SEC))

        self.auto_delete_time=auto_delete_time
        if not self.auto_delete_time:
            return

        date_format = '%Y-%m-%dT%H:%M:%S.%fZ'
        try:
            parsed = datetime.datetime.strptime(auto_delete_time, '%Y-%m-%dT%H:%M:%S.%fZ')
            delta_sec = (parsed - datetime.datetime.utcnow()).total_seconds()
            assert MIN_TTL_SEC <= delta_sec <= MAX_TTL_SEC
        except ValueError:
            raise Exception(
                    'Invalid value for `auto_delete_time`: expected format is `{}`, found `{}`'.format(
                        date_format,
                        auto_delete_time))
        except AssertionError:
            raise Exception(
                    'Invalid value for `auto_delete_time`: time must be between '
                    '{}s and {}s in the future!  Found: {} (which is {} seconds from now)'.format(
                        MIN_TTL_SEC,
                        MAX_TTL_SEC,
                        auto_delete_time,
                        delta_sec,
                        ))


    def _get_jupyter_properties(self):
        jupyter_properties = {}
        jupyter_gcs_dir_property_key = 'dataproc:jupyter.notebook.gcs.dir'
        if (
                self.enable_http_port_access is True and
                isinstance(self.optional_components, list) and
                'JUPYTER' in self.optional_components):

            # Only raise an exception if we are fairly confident the user is using adhoc jupyter
            if self.jupyter_notebook_bucket is None:
                raise Exception(
                    "Project does not have `jupyter_notebook_bucket` property set in stallion config" +
                    "Please set this property in the appropriate adhoc project within BigDataConf")

            gcs_uri = "gs://"
            if self.see_all_notebooks:
                gcs_uri += os.path.join(self.jupyter_notebook_bucket)
            else:
                # No, you are reading this correctly - for some weird reason, Google
                # specifies the bucket twice :-(
                gcs_uri += os.path.join(
                    self.jupyter_notebook_bucket,
                    self.jupyter_notebook_bucket,
                    self.owner)
            jupyter_properties[jupyter_gcs_dir_property_key] = gcs_uri

        return jupyter_properties


    def _add_cluster_config_to_hadoop_properties(self):
        if not self.spark_cluster_key_prefix:
            return

        cluster_config = vars(self)
        prefix = 'spark:' + self.spark_cluster_key_prefix + '.'

        cluster_config_with_spark_keys = {
                prefix + prop:str(val)
                for prop, val in cluster_config.items()
                if prop != 'hadoop_properties'}

        # Also add hadoop properties that are not Spark settings so they are
        # observable in Spark jobs as well.
        for prop, val in cluster_config.get('hadoop_properties', {}).items():
            split = prop.split(':')
            if split[0] != 'spark':
                prop = prefix + split[1]
                cluster_config_with_spark_keys[prop] = str(val)

        self.hadoop_properties.update(cluster_config_with_spark_keys)


    def set_enable_http_port_access(self, enable_http_port_access):
        self.enable_http_port_access = enable_http_port_access


    def set_optional_components(self, optional_components):
        if isinstance(optional_components, list) or optional_components is None:
            self.optional_components = optional_components
        else:
            raise Exception("Optional components must be a list or None")

    def set_see_all_notebooks(self, see_all_notebooks):
        self.see_all_notebooks = see_all_notebooks


    def set_jupyter_notebook_bucket(self, jupyter_notebook_bucket):
        self.jupyter_notebook_bucket = jupyter_notebook_bucket

    @staticmethod
    def determine_spark_application_history_location(spark_application_history_location_argument,
                                               spark_application_history_location_stallion):
        # we take in both an optional argument and Stallion config
        # override with the CLI - set nothing if None in either CLI or Stallion - logs will stay on cluster
        spark_application_history_location = None
        if spark_application_history_location_argument is not None:
            spark_application_history_location = spark_application_history_location_argument
        elif spark_application_history_location_stallion is not None:
            spark_application_history_location = spark_application_history_location_stallion

        return spark_application_history_location

    def _build_autoscaling_policy(self):
        util.validate_autoscaling_policy(self.autoscaling_policy)

        policy = util.build_autoscaling_policy_request_body(self.autoscaling_policy, self.workflow_type.value, self.workflow_name)

        try:
            existing_policy = None
            existing_policy = _get_client().projects().regions().autoscalingPolicies().get(name=_get_resource_name(self.project_id, self.region, policy['id'])).execute()
        except HttpError:
            pass

        try:
            if existing_policy:
                # If policy exists, update it and return policy resource name. Otherwise create new policy
                response = _get_client().projects().regions().autoscalingPolicies().update(name=_get_resource_name(self.project_id, self.region, policy['id']), body=policy).execute()
            else:
                response = _get_client().projects().regions().autoscalingPolicies().create(parent=_get_resource_name(self.project_id, self.region), body=policy).execute()

            return response['name']
        except HttpError as error:
            raise error


    def _master_machine_type_uri(self):
        return util.build_machine_type_uri(
                self.project_id,
                self.zone,
                self.master_machine_type)


    def _worker_machine_type_uri(self):
        return util.build_machine_type_uri(
                self.project_id,
                self.zone,
                self.worker_machine_type)


    def _private_subnetwork_uri(self):
        return 'projects/{project_id}/regions/{region}/subnetworks/hadoop'.format(
                project_id = self.project_id,
                region = self.region)


    def _logs_bucket_path(self, path):
        return 'gs://{logs_bucket}/{path}'.format(
                logs_bucket = self.dataproc_logs_bucket,
                path = path.lstrip('/'))


    def _vpn_subnetwork_uri(self):
        return (
            self.stallion_config.get('sharedvpc_subnetwork_uri') or
            'projects/etsy-sharedvpc-prod/regions/{region}/subnetworks/hadoop-big'.format(
                region=self.region))

    def _build_labels(self):
        # make a local copy of self.labels and then add standard label set.
        labels = dict(iteritems(self.labels))

        labels['contact_email'] = util.sanitize_label(self.contact_email)
        labels['workflow_type'] = self.workflow_type.value
        labels['workflow_owner'] = self.owner
        labels['workflow_name'] = self.workflow_name
        labels['dataproc_common_config_version'] = \
                pkg_resources.get_distribution('etsy_dataproc_common_config').version

        # finally sanitize label values
        return { key: util.sanitize_label(value) for (key, value) in iteritems(labels) }


    def create_placement(self, for_workflow_template=False):
        placement = {
            'clusterName': self.cluster_name,
            'labels': self._build_labels(),
            'config': {
                'gceClusterConfig': {
                    "internalIpOnly": str(self.internal_ip_only).lower(),
                },
                'masterConfig': {
                    'numInstances': '1',
                    'machineTypeUri': self._master_machine_type_uri(),
                    'diskConfig': {
                        'bootDiskSizeGb': self.master_disk_size_gb,
                    }
                },
                'softwareConfig': {
                    'imageVersion': self.image_version,
                    'properties': self._build_properties(),
                }
            }
        }

        if not for_workflow_template:
            # No lifecycle settings for workflow templates
            lifecycle = {}
            if self.idle_delete_ttl_sec:
                lifecycle['idleDeleteTtl'] = str(self.idle_delete_ttl_sec) + 's'

            if self.auto_delete_ttl_sec:
                lifecycle['autoDeleteTtl'] = str(self.auto_delete_ttl_sec) + 's'

            if self.auto_delete_time:
                lifecycle['autoDeleteTime'] = self.auto_delete_time

            if lifecycle:
                placement['config']['lifecycleConfig'] = lifecycle

        if self.num_workers > 0:
            placement['config']['workerConfig'] = {
                    'numInstances': self.num_workers,
                    'machineTypeUri': self._worker_machine_type_uri(),
                    'diskConfig': {
                        'bootDiskSizeGb': self.worker_disk_size_gb,
                    }
                }

        if self.num_preemptible_workers > 0:
            placement['config']['secondaryWorkerConfig'] = {
                'numInstances': self.num_preemptible_workers,
                'machineTypeUri': self._worker_machine_type_uri(),
                'diskConfig': {
                    'bootDiskSizeGb': self.worker_disk_size_gb,
                },
                'isPreemptible': True
            }

        if self.image_uri is not None:
            # If image_uri is set, image_version cannot be set. Unset it here
            del placement['config']['softwareConfig']['imageVersion']
            placement['config']['masterConfig']['imageUri'] = self.image_uri

            # Also, propagate the image_uri to the workers, regular and preemptible
            if self.num_workers > 0:
                placement['config']['workerConfig']['imageUri'] = self.image_uri

            if self.num_preemptible_workers > 0:
                placement['config']['secondaryWorkerConfig']['imageUri'] = self.image_uri

        if self.autoscaling_policy:
            placement['config']['autoscalingConfig'] = {
                'policyUri': self._build_autoscaling_policy()
            }

        if self.zone:
            placement['config']['gceClusterConfig']['zoneUri'] = util.build_zone_uri(
                    self.project_id,
                    self.zone)

        if self.metadata:
            placement['config']['gceClusterConfig']["metadata"] = self.metadata

        if self.service_account:
            # Set the serviceAccount field only if this variable is not empty,
            # because I don't know for sure that dataproc treats a missing parameter
            # equivalently to a parameter that is present but set to None
            placement['config']['gceClusterConfig']['serviceAccount'] = self.service_account

        if self.service_account_scopes:
            placement['config']['gceClusterConfig']['serviceAccountScopes'] = self.service_account_scopes

        placement['config']['gceClusterConfig']['subnetworkUri'] = self.subnetwork_uri

        init_actions = self.init_actions

        if self.use_vpn:
            # prepend init_actions with dns_init, in case any of the other
            # initialization actions require the DNS setup
            init_actions = [ self.dns_init_script_gcs_uri ] + init_actions

        if self.use_data_lake:
            # Add cloud sql init script, so that the metastore service can connect to the Cloud SQL database.
            init_actions = [ self.stallion_config["cloud_sql_proxy_init_script_uri"] ] + init_actions

        placement['config']['initializationActions'] = list(
                map(self.build_init_action, init_actions))

        if self.network_tags:
            placement['config']['gceClusterConfig']['tags'] = self.network_tags


        if (
                self.enable_http_port_access or
                self.image_version >= MIN_VERSION_REQUIRED_FOR_COMPONENT_GATEWAY):
            placement['config']['endpointConfig'] = {
                'enableHttpPortAccess': True,
            }

        if self.optional_components:
            placement['config']['softwareConfig']['optionalComponents'] = self.optional_components

        if not self.image_version.startswith(MAJOR_VERSION_ALLOWING_DATAPROC_INIT_ACTIONS_MODE):
            del placement['config']['softwareConfig']['properties']['dataproc:dataproc.worker.custom.init.actions.mode']

        return placement

    def build_init_action(self, action):
        result = {} if not self.init_action_timeout_sec else {
            'executionTimeout': str(self.init_action_timeout_sec) + 's'
        }

        result['executableFile'] = action

        return result
