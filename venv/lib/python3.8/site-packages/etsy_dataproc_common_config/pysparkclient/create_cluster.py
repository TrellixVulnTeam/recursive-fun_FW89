from .uri_generator import URIGenerator
from etsy_dataproc_common_config.util import execute_partial_function_and_catch_reauth
from etsy_dataproc_common_config.adhoc_config import AdhocConfigLoader
from functools import partial

"""
Given a parser, add an argument group for pyspark specific cluster creation
arguments.

:param parser
:type parser: argparse.ArgumentParser

:return
:type argparse.ArgumentParser
"""
def configure_pyspark_parser_args(parser):
    group = parser.add_argument_group('Arguments to configure versioning for pyspark cluster creation and job_submission.')

    group.add_argument(
            '--repo',
            default='pyspark_ml',
            help='Optional. Repo to retrieve initialization script from. Defaulted to `pyspark_ml`')

    group.add_argument(
            '--package_name',
            default='ps_buzzsaw',
            help='Optional. Package to retrieve initialization script from.')

    group.add_argument(
            '--branch',
            default='master',
            help='Optional. Branch to retrieve initialization script from. Defaulted to `master`')

    group.add_argument(
            '--major_version', type=str,
            default=None,
            help='Optional. Major version of package to retrieve initialization script from. Only available for master branch and by default uses the latest major version.')

    group.add_argument(
            '--patch_version', type=str,
            default=None,
            help='Optional. Minor version of package to retrieve intialization script from. Only available for master branch and by default uses the latest patch version for a given major version.')

    group.add_argument(
            '--bucket_name',
            # default value set in pysparkclient.URIGenerator
            default=None,
            help='Optional. GCS bucket for artifacts organized by git-tagged versions.')

    group.add_argument(
            '--staging_bucket_name',
            default='pyspark-build-staging-87bo',
            help='Optional. GCS bucket that stores artifacts for staging.')

    return parser

def configure_local_init_actions(parser):
    group = parser.add_argument_group('Arguments for managing and optioning pyspark cluster creation.')

    group.add_argument(
            '--ignore_default_init',
            action='store_true',
            help='Optional. Choose to ignore default init script specified through repo, package_name, branch, major_version, patch_version.')

    group.add_argument(
            '--local_init_actions',
            nargs="*",
            default=[],
            help='Optional. Specify local initialization actions separated by whitespace which will be uploaded to a staging folder on GCS.')

    return parser

"""
Add a default pyspark init action script to parsed arguments.

args: command line arguments passed to cluster create script
type: argparse.Namespace

return:
type: argparse.Namespace
"""
def add_pyspark_init_action(args):
    if not args.init_actions_uris:
        args.init_actions_uris = []

    uri_generator = URIGenerator(
        args.repo,
        args.package_name,
        args.branch,
        args.major_version,
        args.patch_version,
        args.bucket_name,
        args.staging_bucket_name
    )

    if not args.ignore_default_init:
        args.init_actions_uris.append(uri_generator.get_artifact_uri("init_script.sh"))

    for action in args.local_init_actions:
        args.init_actions_uris.append(uri_generator.upload_file_to_staging_bucket(action, "init_actions"))

    return args

"""
Add default hadoop properties to pyspark cluster.

args: command line arguments passed to cluster create script
type: argparse.Namespace

return:
type: argparse.Namespace
"""

def add_default_hadoop_properties(properties):
    """
    Get the default properties for Hadoop.
    """
    properties = properties or {}
    hadoop_properties = {"dataproc:dataproc.worker.custom.init.actions.mode":"RUN_BEFORE_SERVICES",
  "core:io.compression.codecs":"org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec",
  "core:io.compression.codec.lzo.class":"com.hadoop.compression.lzo.LzoCodec",
  "spark:spark.dynamicAllocation.enabled":"true",
  "spark:spark.io.compression.codec":"snappy",
  "spark:spark.rdd.compress":"true",
  "capacity-scheduler:yarn.scheduler.capacity.resource-calculator":"org.apache.hadoop.yarn.util.resource.DominantResourceCalculator",
  "spark:spark.hadoop.fs.gs.http.max.retry":"50",
  "core:fs.gs.metadata.cache.enable":"true",
  "dataproc:dataproc.conscrypt.provider.enable":"false"}
    hadoop_properties.update(properties)
    return hadoop_properties

def add_default_custom_image(project_id):
    """
    :param properties: google project id, string
    :return: custom datarproc image uri, string
    """
    config = execute_partial_function_and_catch_reauth(
        partial(AdhocConfigLoader, project_id))
    dataproc_custom_image_for_pyspark = config['dataproc']['dataproc_custom_image_for_pyspark']
    return dataproc_custom_image_for_pyspark
