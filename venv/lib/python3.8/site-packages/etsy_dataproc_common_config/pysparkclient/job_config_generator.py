import re

class PysparkJobConfigGenerator(object):
    """
    Generator class that creates data sent to the dataproc API.
    TODO: dynamically generate paths for archives and driver
    files based on version info from stallion

    driver: driver script
    type: str
    job_args: job args to provide to driver script
    type: arr
    uri_generator: URIGenerator object that contains versioning information
    type: etsy_dataproc_common_config.pysparkclient.URIGenerator
    py_files: python files for job
    type: arr
    jars: jar files for job
    type: arr
    files: files for job
    type: arr
    archives: zipped files to be used
    type: arr
    ignore_default_archive: flag for using default archive artifact or not
    type: bool
    properties: spark/hadoop properties to be used in job
    type: dict
    """

    def __init__(self, driver, job_args, uri_generator, py_files, jars, files, archives, ignore_default_archive, properties, use_local_driver=False, local_archives=[]):
        """
        Constructs a pyspark config generator.
        """
        self.driver = driver
        self.use_local_driver = use_local_driver
        self.job_args = job_args
        self.uri_generator = uri_generator
        self.py_files = py_files
        self.jars = jars
        self.files = files
        self.archives = archives
        self.local_archives = local_archives
        self.ignore_default_archive = ignore_default_archive
        self.properties = self._add_default_spark_properties(properties)

    def get_driver_uri(self):
        """
        Sets the driver file.
        If the local file is being used, the method will upload the file to GCS.
        This means that this method is assumed to only be called once per job run.
        """
        if self.use_local_driver:
            return self.uri_generator.upload_file_to_staging_bucket(self.driver, "driver")

        return self.uri_generator.get_artifact_uri("bin/{}".format(self.driver))

    def _add_default_spark_properties(self, properties):
        """
        Get the default properties for spark.
        """
        properties = properties or {}
        spark_properties = {"spark.pyspark.python":"./venv/bin/python", "spark.pyspark.driver.python" : "/etsy/python_wrap_venv.sh"}
        spark_properties.update(properties)
        return spark_properties

    def _process_archives(self, archives, archive_list, alias_set=set()):
        for archive in archives:
            archive_file, alias = self.uri_generator.get_archive_file_and_alias(archive)
            if alias in alias_set: # if alias is already being used
                raise Exception("Duplicate archive alias: {}.".format(alias))

            if alias is not '':
                alias_set.add(alias)
            archive_list.append(archive)

        return archive_list

    def _generate_archive_list(self):
        """
        Generates the full list of GCS uploaded archives from `archives` and `local_archives`.
        """
        archive_list = []
        alias_set = set()

        self._process_archives(self.archives, archive_list, alias_set)
        local_idx = len(archive_list)

        # check for duplicate aliases before constructing the artifact on the GCS staging area
        self._process_archives(self.local_archives, archive_list, alias_set)

        for i in range(local_idx, len(archive_list)):
            archive_file, alias = self.uri_generator.get_archive_file_and_alias(archive_list[i])
            archive_list[i] = self.uri_generator.upload_file_to_staging_bucket(archive_file, "archives") + alias # uploads local archive to GCS

        return archive_list

    def _get_jars(self):
        """
        Add defaut jars uploaded to GCS. Jar files currently added
         - Lzo decompression jar

        Steps to fetch jar file-
            - Create the cluster using dataproc create cluster command with the correct *debian* version.
            - Login in to the cluster using gcloud --project PROJECT beta compute ssh --zone "us-central1-{a-f}" "CLUSTER_NAME-m" --internal-ip 
            - Copy the jar file from path /usr/lib/hadoop/lib/ to a GCS location (like `gs://etsy-mlinfra-prod-shared-user-scratch-data-6bsh/jars/`)

        TODO : Remove this when google provides solution to automatically access jar in client mode.
        jiraticket : https://jira.etsycorp.com/browse/MLINFRA-1525
        """
        self.jars = self.jars + ["gs://etsy-mlinfra-prod-shared-user-scratch-data-6bsh/jars/hadoop-lzo-0.4.20.jar"]
        return self.jars

    def _get_archives(self):
        """
        If ignore_default_archive is true, then we do not fetch the default archive artifact from GCS.
        Otherwise, we fetch the archive artifact using the uri_generator.

        The alias `#venv` is reserved for the default archive, but if ignore_default_archive
        is true, then one of the other manually specified archives may use it.

        Since this method uploads local archives to GCS, it iss assumed to only be called once.
        """
        if self.ignore_default_archive:
            return self._generate_archive_list()

        default_archive = 'venv.tar.gz'
        default_alias = '#venv'

        for archive in self.archives + self.local_archives:
            if archive.endswith(default_alias):
                raise Exception('Only the default versioned {} can use the {} alias.'.format(default_archive, default_alias))

        archive_list = self._generate_archive_list()
        archive_list.append(self.uri_generator.get_artifact_uri(default_archive) + default_alias)

        return archive_list

    def create_job_config(self):
        """
        Generates template body that is sent to the dataproc API.
        """
        return {
            'mainPythonFileUri': self.get_driver_uri(),
            'args': self.job_args,
            'pythonFileUris': self.py_files,
            'jarFileUris': self._get_jars(),
            'fileUris': self.files,
            'archiveUris': self._get_archives(),
            'properties': self.properties,
        }
