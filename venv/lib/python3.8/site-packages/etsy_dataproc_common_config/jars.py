import os
import re
import operator
from google.cloud import storage
from etsy_dataproc_common_config.util import TimeoutError, retry, timeout

NIGHTLY_DATE_FORMAT_PATTERN = '\d{4}-\d{2}-\d{2}'
DEFAULT_BUILD_JAR_PATH = 'jars/production/builds'
DEFAULT_SPARK_BUILD_JAR_PATH = 'jars/spark'
DEFAULT_NIGHTLY_JAR_PATH = 'jars/production/nightly'
DEFAULT_DEV_JAR_PATH_TEMPLATE = 'jars/development/{branch}/builds'
DEFAULT_JAR_NAME_TEMPLATE = 'jobs-production-assembly-CloudData-{timestamp}-{git_hash}.jar'
DEFAULT_SPARK_JAR_NAME_TEMPLATE = '{module}_{spark_version}_{scala_short}_{timestamp}_{git_hash}.jar'


class JarNotFoundException(Exception):
    pass

class GcsTransport(object):
    """ GcsJarUtil, below, accepts a transport argument in order to allow it to
        be used with multiple different interfaces to GCS: both the
        GoogleCloudStorageHook for use in airflow, and the google.cloud.storage
        package for use outside airflow.  We define this transport class here
        as a wrapper around google.cloud.storage that provides the same API as
        the airflow hook.
    """
    def __init__(self, project_id=None):
        args = {} if not project_id else {'project': project_id}
        self.client = storage.Client(**args)


    def list(self, bucket, prefix):
        bucket = self.client.bucket(bucket)
        return map(operator.attrgetter('name'), bucket.list_blobs(prefix=prefix))


    def upload(self, bucket, object, filename):
        # 1024 * 1024B * 12 = 12MB - Replace the default 100 MB to accomodate slow internet connections.
        chunk_size = 12582912
        blob = self.client.bucket(bucket).blob(object, chunk_size=chunk_size)
        blob.upload_from_filename(filename)

        return True


    def download(self, bucket, object, file):
        blob = self.client.bucket(bucket).blob(object)
        blob.download_to_file(file)


    def exists(self, bucket, object):
        blob = self.client.bucket(bucket).blob(object)
        return blob.exists()


class GcsJarUtil(object):

    @retry(TimeoutError, tries=3)  # On TimeoutErrors, retry up to 3 times
    @timeout(3600)  # Timeout after 1 hour
    def _list_items(self, bucket, prefix):
        """ Shared method for listing objects from the specified bucket and with
            the specified prefix.  Use this instead of interacting with the
            transport directly, to avoid errors relating to different
            transports having different keyword argument orders, for instance
            caused by differences between our GcsTransport and the airflow
            GoogleCloudStorageHook
            :param bucket: the bucket to search
            :type bucket: string
            :param prefix: prefix of the objects to list
            :type prefix: string
        """
        items = list(self.transport.list(
                    bucket=bucket,
                    prefix=prefix))

        assert all(item.startswith(prefix) for item in items), \
                'transport.list() returned results with invalid prefix (expected: {}): {}'.format(
                        prefix,
                        items)

        return items

    def get_jar_timestamp(self, file_path):
        basename = os.path.basename(file_path)
        hit = self.jar_name_regex.match(basename)
        return hit.group('timestamp') if hit else None

    def get_jar_git_hash(self, file_path):
        if not self.git_hash_present:
            return None

        basename = os.path.basename(file_path)
        hit = self.jar_name_regex.match(basename)
        return hit.group('git_hash')

    def get_spark_jar_timestamp(self, file_path):
        basename = os.path.basename(file_path)
        hit = self.spark_jar_name_regex.match(basename)
        return hit.group('timestamp') if hit else None

    def get_spark_jar_git_hash(self, file_path):
        if not self.spark_jar_git_hash_present:
            return None

        basename = os.path.basename(file_path)
        hit = self.spark_jar_name_regex.match(basename)
        return hit.group('git_hash')

    def get_latest_spark_jar(self, module):
        return self.get_latest_spark_build_jar_from_path(self.builds_bucket, self.spark_builds_path, module)

    def get_latest_build_jar(self):
        return self.get_latest_build_jar_from_path(
                self.builds_bucket,
                self.builds_path)

    def get_latest_build_jar_for_branch(self, branch):
        return self.get_latest_build_jar_from_path(
                self.dev_builds_bucket,
                self.dev_builds_path_template.format(branch=branch))

    def get_latest_spark_build_jar_from_path(self, bucket, path_prefix, module):

        builds = self._list_items(bucket, path_prefix)
        module_builds = [b for b in builds if self.spark_jar_name_regex.search(b) and
                         self.spark_jar_name_regex.search(b).group('module') == module]

        return self.get_latest_jar(module_builds, self.get_spark_jar_timestamp)

    def get_latest_local_spark_jar(self, spark_jar_path, module):
        builds = [f for f in os.listdir(spark_jar_path) if self.spark_jar_name_regex.search(f) and
                  self.spark_jar_name_regex.search(f).group('module') == module]
        return max(builds, key=self.get_spark_jar_timestamp) if builds else None

    def get_latest_jar(self, builds, max_function):

        try:
            latest_build = max(builds, key=max_function)
        except ValueError as e:
            # ValueError is raised by max() when no objects are returned.
            # Catch this, but don't catch any other errors (e.g. permissions,
            # bucket does not exist).
            raise JarNotFoundException('No build jars found at path gs://{}/{}'.format(
                self.builds_bucket, self.builds_path))
        else:
            return latest_build

    def get_latest_build_jar_from_path(self, bucket, path_prefix):
        builds = self._list_items(bucket, path_prefix)

        return self.get_latest_jar(builds, self.get_jar_timestamp)

    def get_nightly_jar_date(self, file_path):
        hit = self.nightly_jar_regex.match(file_path)
        return hit.group('date') if hit else None

    def get_latest_nightly_jar(self):
        nightly_jars = self._list_items(self.nightly_bucket, self.nightly_path)

        latest_nightly = None
        try:
            latest_nightly = max(nightly_jars, key = self.get_nightly_jar_date)
        except ValueError as e:
            # ValueError is raised by max() when no objects are returned.
            # Catch this, but don't catch any other errors (e.g. permissions,
            # bucket does not exist).
            raise JarNotFoundException('No nightly jars found at path gs://{}/{}'.format(
                self.nightly_bucket, self.nightly_path))

        return latest_nightly

    def get_nightly_jar(self, date):
        path = os.path.join(self.nightly_path, date)
        jars = self._list_items(self.nightly_bucket, path)

        if len(jars) > 1:
            raise Exception('Multiple items are present in dir: \n{}'.format(
                '\n'.join(jars)))

        if not jars:
            raise JarNotFoundException('Nightly jar not found at location: gs://{}/{}'.format(
                self.nightly_bucket, path))

        jar = jars[0]
        if not self.nightly_jar_regex.match(jar):
            raise Exception('Jar file {} does not match expected pattern {}'.format(
                jar, self.nightly_jar_pattern))

        return jar

    def get_nightly_jar_if_exists(self, date):
        jar = None
        try:
            jar = self.get_nightly_jar(date)
        except JarNotFoundException:
            pass

        return jar

    def __init__(
            self,
            bucket,
            builds_bucket = None,
            builds_path = DEFAULT_BUILD_JAR_PATH,
            spark_builds_path = DEFAULT_SPARK_BUILD_JAR_PATH,
            nightly_bucket = None,
            nightly_path = DEFAULT_NIGHTLY_JAR_PATH,
            dev_builds_bucket = None,
            dev_builds_path_template = DEFAULT_DEV_JAR_PATH_TEMPLATE,
            jar_name_template = DEFAULT_JAR_NAME_TEMPLATE,
            spark_jar_name_template = DEFAULT_SPARK_JAR_NAME_TEMPLATE,
            transport = None):

        self.builds_bucket = builds_bucket or bucket
        self.builds_path = builds_path
        self.spark_builds_path = spark_builds_path

        self.nightly_bucket = nightly_bucket or bucket
        self.nightly_path = nightly_path

        self.dev_builds_bucket = dev_builds_bucket or bucket
        self.dev_builds_path_template = dev_builds_path_template

        self.jar_name_template = jar_name_template
        self.spark_jar_name_template = spark_jar_name_template
        self.transport = transport or GcsTransport()

        if '{timestamp}' not in jar_name_template:
            raise Exception('Invalid jar name template `{}`: missing required '
                'component `{{timestamp}}` !'.format(jar_name_template))

        if '{timestamp}' not in spark_jar_name_template:
            raise Exception('Invalid spark jar name template `{}`: missing required '
                'component `{{timestamp}}` !'.format(jar_name_template))

        self.git_hash_present = '{git_hash}' in jar_name_template

        self.spark_jar_git_hash_present = '{git_hash}' in spark_jar_name_template

        self.jar_name_pattern = jar_name_template.format(
                timestamp = '(?P<timestamp>[0-9]+)',
                git_hash = '(?P<git_hash>[0-9a-f]+)')

        self.jar_name_regex = re.compile(self.jar_name_pattern)

        self.spark_jar_name_pattern = spark_jar_name_template.format(
                timestamp='(?P<timestamp>[0-9]+)',
                git_hash='(?P<git_hash>[0-9a-f]+)',
                module='(?P<module>[a-zA-Z\-_]+)',
                spark_version='(?P<spark_version>[\d]+\.[\d]+\.[\d]+)',
                scala_short='(?P<scala_short>[\d]+\.[\d]+)',
            )

        self.spark_jar_name_regex = re.compile(self.spark_jar_name_pattern)

        self.nightly_jar_pattern = '^{path}/(?P<date>{date_format_pattern})/{jar_name_pattern}$'.format(
                path = self.nightly_path,
                date_format_pattern = NIGHTLY_DATE_FORMAT_PATTERN ,
                jar_name_pattern = self.jar_name_pattern)

        self.nightly_jar_regex = re.compile(self.nightly_jar_pattern)
