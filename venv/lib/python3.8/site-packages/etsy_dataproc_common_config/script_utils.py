import subprocess
import json
from .cluster import \
        EtsyDataprocClusterConfig, \
        MASTER_MAX_ALLOWED_CORES, \
        WORKER_MAX_ALLOWED_CORES
from etsy_dataproc_common_config import util


def machine_type_choices(max_allowed_cores):
    n1_types = [
        'n1-{mem}-{cores}'.format(mem=mem, cores=cores)
        for mem in ('standard', 'highmem')
        for cores in (util.VALID_N1_STANDARD_NUM_CORES if mem == 'standard' else util.VALID_N1_HIGHMEM_NUM_CORES)
        if cores <= max_allowed_cores
    ]
    e2_types = [
        'e2-{mem}-{cores}'.format(mem=mem, cores=cores)
        for mem in ('standard', 'highmem')
        for cores in (util.VALID_E2_STANDARD_NUM_CORES if mem == 'standard' else util.VALID_E2_HIGHMEM_NUM_CORES)
        if cores <= max_allowed_cores
    ]
    n2_types = [
        'n2-{mem}-{cores}'.format(mem=mem, cores=cores)
        for mem in ('standard', 'highmem')
        for cores in (util.VALID_N2_STANDARD_NUM_CORES if mem == 'standard' else util.VALID_N2_HIGHMEM_NUM_CORES)
        if cores <= max_allowed_cores
    ]
    return n1_types + e2_types + n2_types


def comma_separated_list(arg):
    return arg.split(',')


def boolean_string(arg):
    clean_arg = arg.lower()

    if clean_arg == 'true':
        return True

    if clean_arg == 'false':
        return False

    raise Exception('Invalid argument {} (expected true or false)'.format(arg))


def configure_parser(parser, **defaults):
    group = parser.add_argument_group('Arguments to configure the dataproc cluster')
    group.add_argument(
            '--project_id',
            help='Optional. GCP project to create clusters in')

    group.add_argument(
            '--region',
            default='us-central1',
            help='Optional. Region to create clusters in')

    group.add_argument(
            '--zone',
            help='Optional. Zone to create clusters in')

    group.add_argument(
            '--cluster_name',
            help='Optional.  Name for the cluster.')

    group.add_argument(
            '--master_machine_type',
            choices=machine_type_choices(MASTER_MAX_ALLOWED_CORES),
            help='Optional.  GCE instance type to use for master node')

    group.add_argument(
            '--master_disk_size_gb', type=int,
            help='Optional. Master node disk size, in GB')

    group.add_argument(
            '--num_workers', type=int,
            help='Optional. Number of worker nodes')

    group.add_argument(
            '--num_preemptible_workers', type=int,
            help='Optional. Number of preemptible worker nodes')

    group.add_argument(
            '--worker_machine_type',
            choices=machine_type_choices(WORKER_MAX_ALLOWED_CORES),
            help='Optional. GCE instance type to use for worker nodes')

    group.add_argument(
            '--worker_disk_size_gb', type=int,
            help='Optional. Worker node disk size, in GB')

    group.add_argument(
            '--autoscaling_policy',
            help='Optional. Specify autoscaling policy id to use for dataproc cluster.')

    group.add_argument(
            '--network_tags', type=comma_separated_list,
            help='Optional. Network/firewall tags to add to the cluster nodes.  BE CAREFUL'),

    group.add_argument(
            '--use_vpn', type=boolean_string,
            help='Optional. Connect the cluster to the Etsy VPN.  See also: --network_tags if you need firewall access')

    group.add_argument(
            '--internal_ip_only', type=boolean_string, default="true",
            help='Optional. Defaults to `true` and can only be set to `false` if the private VPC is used (in other words, not on the Shared VPC')

    group.add_argument(
            '--dns_init_script_uri',
            help='Optional. DNS init script to use when --use_vpn is enabled')

    group.add_argument(
            '--init_actions_uris', type=comma_separated_list,
            help='Optional. Comma-separated list of GCS URIs of scripts to run on each node of the cluster, at initialization time')

    group.add_argument(
            '--init_action_timeout_sec', type=int,
            help='Optional.  Timeout (in seconds) for the initialization actions. '
                'Use for especially long-lasting operations that may require more '
                'than the default allotment (which is 10 minutes)')

    group.add_argument(
            '--metadata', type=json.loads,
            help='Optional. Json of key-values pairs to set as environment variables')

    image_group = group.add_mutually_exclusive_group()
    image_group.add_argument(
            '--dataproc_image_version',
            help='Optional. Dataproc image version')

    image_group.add_argument(
            '--dataproc_image_uri',
            default=None,
            help=(
                'Optional: If specified, will ue the image to bootstrap dataproc')
    )

    group.add_argument(
            '--dataproc_service_account',
            help='Optional: service account to use for running the DataProc cluster')

    group.add_argument(
            '--hadoop_properties',
            type=json.loads,
            help='Optional: JSON of hadoop properties to pass to dataproc')

    group.add_argument(
            '--canonical_writer',
            action='store_true',
            default=False,
            help='Optional: Whether to run this cluster in canonical-writer mode.  Use with caution; or, preferably, do not use at all.')

    group.add_argument(
            '--idle_delete_ttl_sec',
            default=None,
            type=int,
            help='Optional: Automatically delete cluster if idle for this long')

    group.add_argument(
            '--additional_service_account_scopes',
            default=None,
            type=comma_separated_list,
            help='Optional: Any service account scopes to add to the dataproc worker servicea account beyond the defaults.')

    auto_delete_group = group.add_mutually_exclusive_group()
    auto_delete_group.add_argument(
            '--auto_delete_ttl_sec',
            default=None,
            type=int,
            help='Optional: Automatically delete cluster this many seconds after creation')

    auto_delete_group.add_argument(
            '--auto_delete_time',
            default=None,
            help='Optional: Automatically delete cluster at this time (must be in RFC3339 Z format)')

    group.add_argument(
            '--enable_web_interfaces',
            default=None,
            type=boolean_string,
            help='Optional. Enable http access to specific ports on the cluster from external sources.')

    group.add_argument(
            '--optional_components',
            default=None,
            type=comma_separated_list,
            help=(
                'Optional: The set of optional components to activate on the cluster.' +
                'E.g, ANACONDA,DRUID,JUPYTER,KERBEROS,ZEPPELIN,ZOOKEEPER,PRESTO'))

    group.add_argument(
            '--see_all_notebooks',
            default=None,
            action='store_true',
            help='If this flag is passed, see all Jupyter notebooks within the bucket.')

    group.add_argument(
            '--spark_application_history_location',
            default=None,
            help='A GCS "gs://" location to save Spark Application History')

    group.add_argument(
            '--spark_cluster_key_prefix',
            default=None,
            help=(
                'Optional: A key that will be used to prefix cluster settings'
                ' when viewing a Spark job configuration on Spark History Server')
    )

    group.add_argument(
            '--use_data_lake', type=boolean_string, default="false", 
            help=(
                'Optional: Enables connection access to the data lake metastore.'
                ' Defaults to false.')
    )

    group.set_defaults(**defaults)

    return parser


def cluster_config_from_args(
        args,
        stallion_config,
        workflow_type,
        workflow_name,
        contact_email,
        owner = None):

    return EtsyDataprocClusterConfig(
        project_id = args.project_id,
        workflow_type = workflow_type,
        workflow_name = workflow_name,
        contact_email = contact_email,
        cluster_name = args.cluster_name,
        stallion_config = stallion_config,

        region=args.region,
        zone=args.zone,
        master_machine_type=args.master_machine_type,
        master_disk_size_gb=args.master_disk_size_gb,
        num_workers=args.num_workers,
        num_preemptible_workers=args.num_preemptible_workers,
        worker_machine_type=args.worker_machine_type,
        worker_disk_size_gb=args.worker_disk_size_gb,
        autoscaling_policy=args.autoscaling_policy,
        image_version=args.dataproc_image_version,
        image_uri=args.dataproc_image_uri,
        hadoop_properties=args.hadoop_properties,
        service_account=args.dataproc_service_account,
        canonical_writer=args.canonical_writer,
        use_vpn=args.use_vpn,
        network_tags=args.network_tags,
        dns_init_script_uri=args.dns_init_script_uri,
        init_actions_uris=args.init_actions_uris,
        init_action_timeout_sec=args.init_action_timeout_sec,
        metadata=args.metadata,
        idle_delete_ttl_sec=args.idle_delete_ttl_sec,
        auto_delete_ttl_sec=args.auto_delete_ttl_sec,
        auto_delete_time=args.auto_delete_time,
        additional_service_account_scopes=args.additional_service_account_scopes,
        enable_http_port_access=args.enable_web_interfaces,
        optional_components=args.optional_components,
        see_all_notebooks=args.see_all_notebooks,
        spark_application_history_location=args.spark_application_history_location,
        spark_cluster_key_prefix=args.spark_cluster_key_prefix,
        internal_ip_only=args.internal_ip_only,
        use_data_lake=args.use_data_lake
    )


def get_project_and_user():
    gcloud_output = None
    try:
        gcloud_output = subprocess.check_output([
            'gcloud', 'config', 'list', '--format=json'])

        gcloud_output = json.loads(gcloud_output.strip()).get('core')
        return (gcloud_output.get('project'), gcloud_output['account'])
    except:
        raise Exception('''
        Could not determine current user from `gcloud`!  Please run `gcloud auth login` and try again.
        If you do not have `gcloud` installed, follow the instructions here:
        steps, then try again: https://cloud.google.com/sdk/
        ''')
