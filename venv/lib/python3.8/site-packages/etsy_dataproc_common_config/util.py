import re
import uuid
import time
import signal
from functools import wraps

from past.builtins import basestring
from google.api_core.exceptions import Forbidden
from google.auth.exceptions import DefaultCredentialsError, RefreshError, TransportError
from requests.exceptions import ConnectionError, ReadTimeout
from stallion.exceptions import InvalidComponentException
from subprocess import Popen, CalledProcessError, STDOUT


invalid_chars_re = re.compile('[^a-z0-9_-]')


class TimeoutError(Exception):

    def __init__(self, value = "Timed Out"):
        self.value = value

    def __str__(self):
        return repr(self.value)


def sanitize_label(label_value):
    """ Sanitize label values by replacing characters that GCP does not like
    """
    # First some standard replacements
    label_value = label_value.lower()
    if len(label_value.split('@')) == 2 and label_value.split('@')[-1] == 'etsy.com':
        # treat the label as an email address.  Don't use `-dot-` otherwise because
        # it makes things like version strings illegible
        label_value = label_value.replace('.', '-dot-').replace('@', '-at-')

    # then a catch-all
    label_value = invalid_chars_re.sub('-', label_value)

    # make sure it doesn't exceed 63 chars. Take the last 63 chars because job name
    # labels like com.etsy.scalding.hdfs.VisitValidityChecker have the valuable
    # info at the end of the string
    label_value = label_value[-63:]

    # strip any leading or trailing characters that aren't numbers or letters
    return re.sub(r'^[^a-z0-9]+|[^a-z0-9]+$', '', label_value)


def split_machine_type(machine_type):
    split = machine_type.split('-')
    assert len(split) == 3 and \
            split[1] in [ 'standard', 'highmem' ] and \
            all(c.isdigit() for c in split[2]), \
                'Invalid machine type: {}'.format(machine_type)

    (_, mem_class, num_cores_str) = split
    return (mem_class == 'highmem', int(num_cores_str))


def validate_worker_config(num_workers, num_preemptible_workers):
    if num_workers == 0 and num_preemptible_workers > 0:
        raise ValueError('Premptible workers are not allowed in a single node cluster (requested {})'.format(num_preemptible_workers))


# Store the valid core counts for the standard and highmem instance classes,
# and check provided node configs against these.
# Available machine types can be found here: https://cloud.google.com/compute/docs/machine-types
VALID_N1_STANDARD_NUM_CORES = [1, 2, 4, 8, 16, 32, 64, 96]
VALID_N1_HIGHMEM_NUM_CORES = [2, 4, 8, 16, 32, 64, 96]

VALID_N2_STANDARD_NUM_CORES = [2, 4, 8, 16, 32, 48, 64, 80]
VALID_N2_HIGHMEM_NUM_CORES = [2, 4, 8, 16, 32, 48, 64, 80]

VALID_E2_STANDARD_NUM_CORES = [2, 4, 8, 16]
VALID_E2_HIGHMEM_NUM_CORES = [2, 4, 8, 16]


def validate_node_config(machine_type, max_allowed_cores):
    (highmem, num_cores) = split_machine_type(machine_type)

    valid_standard_num_cores = VALID_N1_STANDARD_NUM_CORES
    valid_highmem_num_cores = VALID_N1_HIGHMEM_NUM_CORES

    if machine_type.startswith('e2'):
        valid_standard_num_cores = VALID_E2_STANDARD_NUM_CORES
        valid_highmem_num_cores = VALID_E2_HIGHMEM_NUM_CORES

    valid_core_count = valid_standard_num_cores if not highmem else \
        valid_highmem_num_cores

    if num_cores not in valid_core_count:
        raise ValueError('Invalid core count {} for node type {}: valid counts are {}'.format(
                    num_cores,
                    'standard' if not highmem else 'highmem',
                    valid_core_count))

    if num_cores > max_allowed_cores:
        raise ValueError(
                'Sorry, we do not allow the use of instances with more than {} cores at this time.  Consider using a larger cluster with ffewer cores per node.'.format(
                    max_allowed_cores))


def build_machine_type_uri(project_id, zone, base_machine_type):
    if not zone:
        return base_machine_type

    return "https://www.googleapis.com/compute/v1/projects/{project_id}/zones/{zone}/machineTypes/{base_machine_type}".format(
            **locals())


def build_zone_uri(project_id, zone):
    if not zone:
        return None

    return "https://www.googleapis.com/compute/v1/projects/{project_id}/zones/{zone}".format(
            **locals())

# We define a method to check any user-provided properties to make sure that
# their keys fit the pattern
#  prefix:config.path.xyz,
# where prefix must be one of a fixed set of values corresponding to the
# various hadoop/spark config files. See:
# https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/cluster-properties
DATAPROC_PROPERTY_KEY_PREFIXES = frozenset([
    'core', 'hdfs', 'mapred', 'distcp', 'yarn', 'capacity-scheduler',
    'hive', 'pig', 'spark', 'mapred-env', 'yarn-env', 'spark-env', 'dataproc'])


def validate_property_keys(properties):
    invalid_keys = [ key for key in properties if
            key.split(':')[0] not in DATAPROC_PROPERTY_KEY_PREFIXES ]

    if not invalid_keys:
        return

    raise ValueError(
            '''Invalid `properties` arg: {} (keys {} are invalid.  Must be of
            the form `prefix:path.to.config.key`, where `prefix` is one of {})'''.format(
                properties,
                invalid_keys,
                DATAPROC_PROPERTY_KEY_PREFIXES))

GS_REGEX = re.compile('gs://([^/]+)/(.*)')


def validate_gcs_uri(uri):
    if GS_REGEX.match(uri):
        return

    raise ValueError('Not a GCS URI: {}'.format(uri))


def validate_region(region):
    # we only support us-central1 right now
    if region != 'us-central1':
        raise ValueError('Invalid region `{}`: only us-central1 is supported at this time'.format(region))


def validate_zone(zone):
    supported_zones = 'us-central1-[abcf]'
    if zone is not None and not re.match(supported_zones, zone):
        raise ValueError('Invalid zone `{}`: supported zones include {}'.format(
            zone,
            supported_zones))


def validate_cluster_init_actions(init_actions):
    if not isinstance(init_actions, list) or any(
            not isinstance(item, basestring) for item in init_actions):
        raise TypeError(
                'Invalid `init_actions` argument: {} (expected a list of strings)'.format(
                    init_actions))

    for action in init_actions:
        validate_gcs_uri(action)


def validate_autoscaling_policy(policy):
    required_fields = ['worker_config', 'min_instances', 'max_instances', 'scale_down_factor', 'graceful_decommission_timeout', 'scale_up_factor', 'basic_algorithm', 'yarn_config']

    try:
        relevant_fields = list(policy.keys()) + list(policy.get('worker_config').keys()) + list(policy.get('basic_algorithm').keys()) + list(policy.get('basic_algorithm').get('yarn_config').keys())

        if set(required_fields).issubset(relevant_fields):
            return None
        else:
            raise ValueError('The following fields are required: {}'.format(required_fields))
    except AttributeError:
        raise AttributeError('The following fields are required: {}'.format(required_fields))



def execute_partial_function_and_catch_reauth(partial_func):
    results = None

    try:
        print("Calling partial function {} with args {}".format(
            partial_func.func.__name__,
            partial_func.args))
        results = partial_func()
    except RefreshError as e:
        cmds = [
            ['gcloud', 'auth', 'login'],
            ['gcloud', 'auth', 'application-default', 'login'],
        ]
        for cmd in cmds:
            print("Caught RefreshError... request re-authentication with cmd: {}".format(" ".join(cmd)))
            popen = Popen(cmd, stderr=STDOUT, universal_newlines=True, bufsize=1)
            return_code = popen.wait()

            if return_code != 0:
                raise CalledProcessError(return_code, cmd)

        print("Re-calling partial function {} with args {}".format(
            partial_func.func.__name__,
            partial_func.args))
        results = partial_func()

    return results


def build_autoscaling_policy_request_body(policy, workflow_type, workflow_name):
    policy_id = '{}-{}-{}'.format(str(uuid.uuid1())[0:8], workflow_type, workflow_name)

    body = {'workerConfig': {}, 'secondaryWorkerConfig': {}, 'basicAlgorithm': {'yarnConfig': {}}}

    body['id'] = policy_id[0:48]
    body['workerConfig']['minInstances'] = policy.get('worker_config').get('min_instances')
    body['workerConfig']['maxInstances'] = policy.get('worker_config').get('max_instances')
    body['basicAlgorithm']['yarnConfig']['scaleDownFactor'] = policy.get('basic_algorithm').get('yarn_config').get('scale_down_factor')
    body['basicAlgorithm']['yarnConfig']['gracefulDecommissionTimeout'] = policy.get('basic_algorithm').get('yarn_config').get('graceful_decommission_timeout')
    body['basicAlgorithm']['yarnConfig']['scaleUpFactor'] = policy.get('basic_algorithm').get('yarn_config').get('scale_up_factor')

    if policy.get('worker_config').get('weight'):
        body['workerConfig']['weight'] = policy.get('worker_config').get('weight')

    if policy.get('secondary_worker_config'):
        if policy.get('secondary_worker_config').get('min_instances'):
            body['secondaryWorkerConfig']['minInstances'] = policy.get('secondary_worker_config').get('min_instances')
        if policy.get('secondary_worker_config').get('max_instances'):
            body['secondaryWorkerConfig']['maxInstances'] = policy.get('secondary_worker_config').get('max_instances')
        if policy.get('secondary_worker_config').get('weight'):
            body['secondaryWorkerConfig']['weight'] = policy.get('secondary_worker_config').get('weight')

    if policy.get('basic_algorithm').get('yarn_config').get('scale_down_min_worker_fraction'):
        body['basicAlgorithm']['yarnConfig']['scaleDownMinWorkerFraction'] = policy.get('basic_algorithm').get('yarn_config').get('scale_down_min_worker_fraction')

    if policy.get('basic_algorithm').get('yarn_config').get('scale_up_min_worker_fraction'):
        body['basicAlgorithm']['yarnConfig']['scaleUpMinWorkerFraction'] = policy.get('basic_algorithm').get('yarn_config').get('scale_up_min_worker_fraction')

    if policy.get('basic_algorithm').get('cooldown_period'):
        body['basicAlgorithm']['cooldownPeriod'] = policy.get('basic_algorithm').get('cooldown_period')

    return body


def call_and_catch_cli_func_with_troubleshooting_steps(partial_func):
    results = None
    try:
        results = partial_func()

    except Forbidden as e:
        print(e)
        print('\n\nTo resolve this issue, please set the following environment variable:')
        print('export GOOGLE_APPLICATION_CREDENTIALS=/home/<LDAP>/.config/gcloud/legacy_credentials/<LDAP>@etsy.com/adc.json')
        print('Pro-tip: You can add this to your ~/.bashrc')

    except RefreshError as e:
        print(e)
        print('\n\nTo resolve this issue, you may need to run the following:')
        print('gcloud auth login')
        print('gcloud auth application-default login')
        print('Pro-tip: You can add the following to your ~/.bashrc, source it, and then simply run gauth')
        print(
"""
gcloud_auth() {
    gcloud auth login
    gcloud auth application-default login
}
alias gauth=gcloud_auth
"""
)

    except (ConnectionError, ReadTimeout, TransportError) as e:
        print(e)
        print('\n\nSlow internet speeds were detected and a read / write timeout exception occurred.')
        print('Please check your internet upload speeds and ensure that they are above 10mbps')

    except EnvironmentError as e:
        print(e)
        print('\n\nTo resolve this issue, please set the following environment variable:')
        print('export GCLOUD_PROJECT=<PROJECT_NAME>')
        print('For ad-hoc purposes, you should use `etsy-adhoc-jobs-prod`')
        print('Pro-tip: You can add this to your ~/.bashrc')

    except InvalidComponentException as e:
        print(e)
        print('\n\nYou are running an older version of Stallion.')
        print('Please re-install the latest version of workflow-templates or dataproc-workflows')
        print('You can do this by ensuring your venv is activated and running one of the following:')
        print('If you are using create_dataproc_cluster or create_pyspark_cluster CLI tools:')
        print('`pip install --upgrade --upgrade-strategy=eager etsy-dataproc-common-config`')
        print('If you are using gcp_run_scalding or gcp_run_spark CLI tools:')
        print('`pip install --upgrade --upgrade-strategy=eager workflow-templates`')

    except DefaultCredentialsError as e:
        print(e)
        print("\n\nTo resolve this issue, run the following bash command.")
        print("`unset GOOGLE_APPLICATION_CREDENTIALS`")

    return results


def retry(ExceptionToCheck, tries=4, delay=3, backoff=2, logger=None):
    """Retry calling the decorated function using an exponential backoff.

    http://www.saltycrane.com/blog/2009/11/trying-out-retry-decorator-python/
    original from: http://wiki.python.org/moin/PythonDecoratorLibrary#Retry

    :param ExceptionToCheck: the exception to check. may be a tuple of
        exceptions to check
    :type ExceptionToCheck: Exception or tuple
    :param tries: number of times to try (not retry) before giving up
    :type tries: int
    :param delay: initial delay between retries in seconds
    :type delay: int
    :param backoff: backoff multiplier e.g. value of 2 will double the delay
        each retry
    :type backoff: int
    :param logger: logger to use. If None, print
    :type logger: logging.Logger instance
    """
    def deco_retry(f):

        @wraps(f)
        def f_retry(*args, **kwargs):
            mtries, mdelay = tries, delay
            while mtries > 1:
                try:
                    return f(*args, **kwargs)
                except ExceptionToCheck as e:
                    msg = "%s, Retrying in %d seconds..." % (str(e), mdelay)
                    if logger:
                        logger.warning(msg)
                    else:
                        print(msg)
                    time.sleep(mdelay)
                    mtries -= 1
                    mdelay *= backoff
            return f(*args, **kwargs)

        return f_retry  # true decorator

    return deco_retry


def timeout(seconds_before_timeout):
    """
    Copied from:
    https://www.saltycrane.com/blog/2010/04/using-python-timeout-decorator-uploading-s3/
    """

    def decorate(f):

        def handler(signum, frame):
            raise TimeoutError()

        def new_f(*args, **kwargs):
            old = signal.signal(signal.SIGALRM, handler)
            signal.alarm(seconds_before_timeout)
            try:
                result = f(*args, **kwargs)
            finally:
                # reinstall the old signal handler
                signal.signal(signal.SIGALRM, old)
                # cancel the alarm
                signal.alarm(0)
            return result
        new_f.__name__ = f.__name__
        return new_f
    return decorate
