import json
import os
import time
from argparse import ArgumentParser

from etsy_dataproc_common_config import EtsyDataprocClusterConfig, WorkflowType, script_utils
from etsy_dataproc_common_config.adhoc_config import AdhocConfigLoader
from etsy_dataproc_common_config.util import execute_partial_function_and_catch_reauth
from functools import partial
from stallion.auth import Authorizer
from .pysparkclient.create_cluster import configure_pyspark_parser_args, configure_local_init_actions


def configure_default_args(parser, default_project_id):
    return script_utils.configure_parser(
            parser,
            project_id=default_project_id,
            num_workers=2,
            )

def get_pyspark_args(default_project_id):
    parser = ArgumentParser()
    parser = configure_default_args(parser, default_project_id)
    parser = configure_pyspark_parser_args(parser)
    parser = configure_local_init_actions(parser)
    return parse_args(parser)

def get_args(default_project_id):
    parser = ArgumentParser()
    parser = configure_default_args(parser, default_project_id)
    return parse_args(parser)

def parse_args(parser):
    args = parser.parse_args()

    if not args.project_id:
        raise Exception("""
        No project_id found, either in the command-line args or in your gcloud defaults.
        Please either provide a --project_id argument, or set a default project via:
        gcloud config set project <my-project-id>
        """)

    # Set environment variable for the user
    os.environ['GOOGLE_CLOUD_PROJECT'] = args.project_id

    return args

def create_cluster_config(args, user_email):
    print("Getting adhoc config loader")

    config = execute_partial_function_and_catch_reauth(
        partial(AdhocConfigLoader, args.project_id))
    print('Retrieving user credentials...')

    user_id = user_email.split('@')[0]
    workflow_name = '{}-{}'.format(user_id, int(time.time()))

    cluster_config = script_utils.cluster_config_from_args(
            args,
            config['dataproc'],
            WorkflowType.ADHOC,
            workflow_name = workflow_name,
            contact_email = user_email)

    print('Cluster configuration blob: {}'.format(
        json.dumps(cluster_config.create_placement())))

    return cluster_config


def dataproc_cluster_create(args, cluster_config):
    print('Calling Dataproc API to create cluster...')

    session = execute_partial_function_and_catch_reauth(
        partial(Authorizer.read_write().get_access_session))

    response = session.post(
            'https://dataproc.googleapis.com/v1beta2/projects/{}/regions/{}/clusters/'.format(
                args.project_id,
                args.region),
            json=cluster_config.create_placement())

    if response.status_code != 200:
        raise Exception(
                'Failed to create cluster: status code {}; content {}'.format(
                    response.status_code,
                    response.content))

    response_body = response.json()

    cluster_link = (
        "https://console.cloud.google.com/dataproc/clusters/" +
        response_body['metadata']['clusterName'] +
        '?project={project}&region={region}'.format(project=args.project_id, region=args.region))
    print(
        'Cluster creation request submitted successfully.\n' +
        '\tCluster name is `{}`\n\tCluster uuid is {}\n\tCurrent state is: {}\n\tCluster link is: {}'.format(
        response_body['metadata']['clusterName'],
        response_body['metadata']['clusterUuid'],
        response_body['metadata']['status']['state'],
        cluster_link))

    # Get additional cluster details using the same request session
    cluster_response = session.get(
            'https://dataproc.googleapis.com/v1beta2/projects/{}/regions/{}/clusters/{}'.format(
                args.project_id,
                args.region,
                response_body['metadata']['clusterName']))

    if cluster_response.status_code != 200:
        raise Exception(
            'Failed to extract cluster details: status code {}; content {}'.format(
                cluster_response.status_code,
                cluster_response.content))

    cluster_details = json.loads(cluster_response.content)
    web_interface_links = cluster_details.get('config', {}).get('endpointConfig', {}).get('httpPorts', {})

    print(
        'Web Interface Links (please wait until the cluster is RUNNING to access):\n{}'.format(
            json.dumps(web_interface_links, indent=4)))
