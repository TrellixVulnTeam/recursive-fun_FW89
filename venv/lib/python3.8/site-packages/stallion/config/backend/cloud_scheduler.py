
import base64
import json
import logging
import marshmallow as ma
from operator import itemgetter
import re
import subprocess

from stallion import constants, exceptions, util
from stallion.config.backend.base import ConfigBackendBase
from stallion.schemas.cloud_scheduler import CloudSchedulerSchema
from stallion.schemas.base import StrictSchema

# TODO not here but where the BackendConfigs are loaded: see how it might be
# extended so that the jobs could be stored one per file -- this would
# probably look like some sort of recursive scan of a nested directory rather
# than looking for that one settings file, were we to do the thing right --
# need to think about whether implementing that is worthwhile.

class CloudSchedulerConfigBackendPropertiesSchema(StrictSchema):
    """ The schema for the CloudSchedulerConfigBackend properties."""

    # The id of the project in which the Cloud Scheduler Jobs are hosted.
    project_id = ma.fields.String(required=True)
    # A prefix for the names of jobs managed by this ConfigBackend.
    job_group = ma.fields.String(required=True)


class CloudSchedulerConfigBackend(ConfigBackendBase):
    """ A config backend that serves as a management layer over Cloud Scheduler.
        Note that this occupies a kind of funky corner of the cloud resources
        deployment space. The mental model this commenter uses is:
        Cloud Scheduler is a service whose CI someone else manages.
        The configuration has been left to us. Rather than putting a file on GCS
        or updating a KV store, to update the configuration of this service we
        need to make the appropriate API calls such that the configuration of
        the component (subject to some mapping function) equals the
        configuration returned by the Cloud Scheduler service when asked to list
        its jobs. The fact that the way to achieve this result is the creation,
        updating, and deletion of jobs is not really important from the
        perspective of this piece of software. What matters is achieving parity
        between the configs.

        :param properties: the properties for this config backend instance. Must
            match the schema defined by
            CloudSchedulerConfigBackendPropertiesSchema
        :type properties: dict
    """

    name = "cloud_scheduler"
    properties_schema = CloudSchedulerConfigBackendPropertiesSchema()

    MISUSE_MSG = "This backend should not be used with components that have " \
        "code versions."

    def __init__(self, properties):
        super(CloudSchedulerConfigBackend, self).__init__(properties)
        self.project_id = self.properties['project_id']
        self.job_group = self.properties['job_group']


    def delete_component_version(
            self,
            component_name,
            component_version,
            session=None):
        """ This backend should only be used with components that only allow
            for noop deployment and thus the deletion of a specific component
            version does not, at least in the current implementation, make
            sense. The concept of versioning in Stallion is coupled with the
            notion of a code artifact. If someone tries to call this method,
            we raise a NotImplementedError.
        """
        raise NotImplementedError(self.MISUSE_MSG)


    def list_component_versions(self, component_name, session=None):
        """ This backend should only be used with components that only allow
            for noop deployment and thus the listing of component versions
            does not, at least in the current implementation, make sense.
            The concept of versioning in Stallion is coupled with the notion of
            a code artifact. If someone tries to call this method, we raise a
            NotImplementedError.
        """
        raise NotImplementedError(self.MISUSE_MSG)


    def get_component_config(
            self,
            component_name,
            component_version,
            **kwargs):
        """ Retrieve the configuration for the specified component.

            :param component_name: the name of the component
            :type component_name: str
            :param component_version: the version of the component
            :type component_version: str
            :return the component's configuration, in its fully nested form
            :rtype dict
        """

        # NOTE: This method is only used at runtime by the CachingConfigLoader,
        # and so in the current implementation where only components limited to
        # noop deployers can use this backend this method will never be called.
        # But. That being said. Since it's not that hard to implement, and since
        # I could imagine future use cases for it, as long as the caller passes
        # a version of NO_CODE_VERSION, we return.
        if component_version != constants.NO_CODE_VERSION:
            raise exceptions.InvalidComponentException(self.MISUSE_MSG)

        jobs = self._retrieve_jobs()
        return {
            'jobs': jobs
        }


    # TODO see if the inclusion of **kwargs allows strict to be deleted from the
    # method signature.
    def _write_component_config(
            self,
            component_name,
            component_version,
            item,
            strict,
            **kwargs):
        """ Write the provided component config into the backend data store.

            :param component_name: the name of the component
            :type component_name: str
            :param component_version: the version of the component
            :type component_version: str
            :param item: the config to write. This dict must conform to the
                CloudSchedulerSchema. See more in note below.
            :type item: dict
            :param strict: the strict parameter is ignored in this
                implementation of the ConfigBackendBase class, which will always
                consider the provided item to be the exact record of what should
                be written to Cloud Scheduler
            :type strict: bool

            :return nothing
            :rtype NoneType
        """
        # This backend only works with a configuration that adheres to the
        # CloudSchedulerSchema. Whether allowed backends should be configured on
        # the component in a manner similar to deployers is worth some thought,
        # but for now load the item into the schema and if there are errors
        # throw an exception. Also throw an exception if a component version
        # other than NO_CODE_VERSION is passed, as this is a pretty good
        # indicator something is misconfigured.
        if component_version != constants.NO_CODE_VERSION:
            raise exceptions.InvalidComponentException(self.MISUSE_MSG)

        schema = CloudSchedulerSchema()
        loaded = schema.load(item['config'])

        if loaded.errors:
            raise Exception(loaded.errors)

        jobs = loaded.data['jobs']

        self._reconcile_state(jobs)


    # NOTE: re: the lack of exception-handling in this method:
    # If partial successes are acceptable in Terraform, they're certainly
    # acceptable here. Forreal tho, adding retries to the API calls is TODO,
    # and everything else I could imagine that might cause this method to bomb
    # is either a) totally blocking (perms on Cloud Scheduler are lost, Jenky
    # has his internet access revoked by ma & pa) or b) transient (GCP outage).
    # That extremely thorough case analysis + the fact that with this
    # implementation retries after partial successes will result in full
    # successes leads me to the conclusion that trying to implement some kind
    # of revert logic would cause more harm than good.
    def _reconcile_state(self, jobs):
        """ Make the list of jobs upstream with names that start with
            self.job_group match the list of jobs provided to this method.
            Notably, this includes deleting jobs upstream that are not present
            in the provided list.

            :param jobs: the name of the component
            :type jobs: list of dict

            :return nothing
            :rtype NoneType
        """

        names_to_jobs = util.keyby(
            jobs,
            itemgetter('name'),
            'Duplicate job names found!')

        deployed_jobs = self._retrieve_jobs()

        names_to_deployed_jobs = util.keyby(
            deployed_jobs,
            itemgetter('name'),
            'Duplicate deployed job names found!')

        # NOTE: job['name'] serves as PK
        jobs_to_update = self._jobs_to_update(names_to_jobs,
                                              names_to_deployed_jobs)

        logging.info("Updating the following jobs: %s from project: %s",
            self._format_comma_separated_job_names(jobs_to_update),
            self.project_id)

        for job in jobs_to_update:
            self._update_job(job)

        # NOTE: deletes preceding creates is intentional. A name change in the
        # config is implemented as a delete + create. There shouldn't be a time
        # when both the old and new job are live.
        jobs_to_delete = self._jobs_to_delete(names_to_jobs,
                                              names_to_deployed_jobs)

        logging.info("Deleting the following jobs: %s from project: %s",
            self._format_comma_separated_job_names(jobs_to_delete),
            self.project_id)

        for job in jobs_to_delete:
            self._delete_job(job['name'])

        jobs_to_create = self._jobs_to_create(names_to_jobs,
                                              names_to_deployed_jobs)

        logging.info("Creating the following jobs: %s in project: %s",
            self._format_comma_separated_job_names(jobs_to_create),
            self.project_id)

        for job in jobs_to_create:
            self._create_job(job)


    def _retrieve_jobs(self):
        """ Retrieve the list of jobs in the GCP project with id self.project_id
            with names that start with self.job_group. Transform the results to
            conform to the CloudSchedulerSchema for easy comparison.
        """
        gcp_csjs = self._list_jobs()
        return [
            self._gcp_cloud_scheduler_job_to_stallion_cloud_scheduler_job(
                gcp_csj)
            for gcp_csj in gcp_csjs
            if self._format_local_job_name(gcp_csj['name']) \
                .startswith(self.job_group)
            ]


    def _build_gcloud_create_cloud_scheduler_pubsub_job_args(self, job):
        """ Build gcloud command to create a Cloud Scheduler job targeting
            Pub/sub in the GCP project with id self.project_id.

            :param job: the name of the component
            :type job: dict

            :return nothing
            :rtype NoneType
        """
        args = [
            'gcloud',
            'scheduler',
            'jobs',
            '--project',
            self.project_id,
            'create',
            'pubsub',
            self._add_job_group_prefix(job['name']),
            '--schedule',
            job['schedule'],
            '--topic',
            job['pub_sub_target']['topic_name'],
            '--message-body',
            job['pub_sub_target']['data']
        ]

        if 'max_retry_attempts' in job:
            args.extend(['--max-retry-attempts', str(job['max_retry_attempts'])])

        if 'min_backoff' in job:
            args.extend(['--min-backoff', str(job['min_backoff']) + 's'])

        if 'max_backoff' in job:
            args.extend(['--max-backoff', str(job['max_backoff']) + 's'])

        return args


    def _build_gcloud_delete_cloud_scheduler_job_args(self, job_name):
        """ Build gcloud command to delete a Cloud Scheduler job in the GCP
            project with id self.project_id and the provided name.

            :param job: the name of the component
            :type job: dict

            :return nothing
            :rtype NoneType
        """

        args = [
            'gcloud',
            'scheduler',
            'jobs',
            '--project',
            self.project_id,
            'delete',
            self._add_job_group_prefix(job_name),
            '--quiet'
        ]
        return args


    def _build_gcloud_update_cloud_scheduler_pubsub_job_args(self, job):
        """ Build gcloud command to create a Cloud Scheduler job targeting
            Pub/sub in the GCP project with id self.project_id.

            :param job: the name of the component
            :type job: dict

            :return nothing
            :rtype NoneType
        """
        args = [
            'gcloud',
            'scheduler',
            'jobs',
            '--project',
            self.project_id,
            'update',
            'pubsub',
            self._add_job_group_prefix(job['name']),
            '--schedule',
            job['schedule'],
            '--topic',
            job['pub_sub_target']['topic_name'],
            '--message-body',
            job['pub_sub_target']['data'],
            '--verbosity',
            'debug',
        ]

        if 'max_retry_attempts' in job:
            args.extend(['--max-retry-attempts', str(job['max_retry_attempts'])])
        else:
            args.append('--clear-max-retry-attempts')

        if 'min_backoff' in job:
            args.extend(['--min-backoff', str(job['min_backoff']) + 's'])
        else:
            args.append('--clear-min-backoff')

        if 'max_backoff' in job:
            args.extend(['--max-backoff', str(job['max_backoff']) + 's'])
        else:
            args.append('--clear-max-backoff')

        return args


    def _build_gcloud_list_cloud_scheduler_jobs_args(self):
        """ Build gcloud command to create a Cloud Scheduler job targeting
            Pub/sub in the GCP project with id self.project_id.

            :param job: the name of the component
            :type job: dict

            :return nothing
            :rtype NoneType
        """
        args =[
            'gcloud',
            'scheduler',
            'jobs',
            'list',
            '--format=json',
            '--project',
            self.project_id]
        return args


    def _jobs_to_update(self, names_to_jobs, names_to_deployed_jobs):
        """ Compare jobs from config to deployed jobs and return the jobs from
            config whose configurations need to be applied upstream.

            :param names_to_jobs: the names of jobs from config mapped to said
                jobs
            :type job: dict
            :param names_to_deployed_jobs: the names of jobs retrieved from
                upstream mapped to said jobs
            :type job: dict

            :return the jobs from config whose configurations need to be applied
                upstream
            :rtype list of dict
        """
        job_names = list(names_to_jobs.keys())
        deployed_job_names = list(names_to_deployed_jobs.keys())
        intersection = set(job_names).intersection(set(deployed_job_names))

        # Restructuring this to log to see why this is working on my laptop
        # but not when called by Jenkins - can put back into a list comprehension
        # if we feel like it once we get some useful logs from Jenkins
        jobs_to_update = []

        for name in intersection:
            if names_to_jobs[name] != names_to_deployed_jobs[name]:
                logging.info(
                    "Config value %s does not equal upstream value %s",
                    str(names_to_jobs[name]),
                    str(names_to_deployed_jobs[name]))
                jobs_to_update.append(names_to_jobs[name])

        return jobs_to_update


    def _jobs_to_delete(self, names_to_jobs, names_to_deployed_jobs):
        """ Compare jobs from config to deployed jobs and return the deployed
            ones that need to be deleted.

            :param names_to_jobs: the names of jobs from config mapped to said
                jobs
            :type job: dict
            :param names_to_deployed_jobs: the names of jobs retrieved from
                upstream mapped to said jobs
            :type job: dict

            :return the deployed jobs that need to be deleted
            :rtype list of dict
        """
        job_names = list(names_to_jobs.keys())
        deployed_job_names = list(names_to_deployed_jobs.keys())
        difference = set(deployed_job_names).difference(set(job_names))
        return [names_to_deployed_jobs[name] for name in difference]


    def _jobs_to_create(self, names_to_jobs, names_to_deployed_jobs):
        """ Compare jobs from config to deployed jobs and return the jobs from
            config that need to be created.

            :param names_to_jobs: the names of jobs from config mapped to said
                jobs
            :type job: dict
            :param names_to_deployed_jobs: the names of jobs retrieved from
                upstream mapped to said jobs
            :type job: dict

            :return the jobs from config that need to be created
            :rtype list of dict
        """
        job_names = list(names_to_jobs.keys())
        deployed_jobs_names = list(names_to_deployed_jobs.keys())
        difference = set(job_names).difference(set(deployed_jobs_names))
        return [names_to_jobs[name] for name in difference]


    def _format_local_job_name(self, full_name):
        """ Google is inconsistent in what it returns when you ask for a Cloud
            Scheduler Job name, but this method is for extracting the shorter
            version out of the long one, ex:
            projects/[project-id]/locations/us-central1/jobs/test-script-job ->
            test-script-job
        """
        return full_name.split('/')[-1]


    def _strip_job_group_prefix(self, local_name):
        """ Remove the job_group prefix, ex: if
            local_name = data_plat_jobs_test_script_job
            and
            self.job_group = data_plat_jobs_
            this method returns test_script_job
        """
        if not local_name.startswith(self.job_group):
            raise Exception("Job name: {} doesn't have prefix: {} and thus " \
                "said prefix cannot be stripped.".format(
                    local_name,
                    self.job_group))
        return local_name[len(self.job_group):]


    def _add_job_group_prefix(self, local_name):
        """ return provided name prefixed by self.job_group """
        return '{}{}'.format(self.job_group, local_name)


    def _format_local_topic_name(self, full_name):
        """ Google is inconsistent in what it returns when you ask for a Pubsub
            Topic name, but this method is for extracting the shorter version
            out of the long one, ex:
            projects/[project_id]/topics/test-topic ->
            test-topic
        """
        return full_name.split('/')[-1]


    def _format_comma_separated_job_names(self, jobs):
        return ','.join([j['name'] for j in jobs])


    def _create_job(self, job):
        """ Create the provided Google Cloud Scheduler Job using the gcloud
            API. Note that this implementation was chosen over the python sdk
            b/c at time of writing, the python sdk didn't work.
        """
        self._run_cmd(
            self._build_gcloud_create_cloud_scheduler_pubsub_job_args(job))


    def _update_job(self, job):
        """ Update the provided Google Cloud Scheduler Job using the gcloud
            API. Note that this implementation was chosen over the python sdk
            b/c at time of writing, the python sdk didn't work.
        """
        self._run_cmd(
            self._build_gcloud_update_cloud_scheduler_pubsub_job_args(job))


    def _delete_job(self, job_name):
        """ Delete the Google Cloud Scheduler Job with the provided name using
            the gcloud API. Note that this implementation was chosen over the
            python sdk b/c at time of writing, the python sdk didn't work.
        """
        self._run_cmd(
            self._build_gcloud_delete_cloud_scheduler_job_args(job_name))


    def _list_jobs(self):
        """ Return a list the Google Cloud Scheduler Jobs retrieved using the
            gcloud API. Note thatthis implementation was chosen over the
            python sdk b/c at time of writing, the python sdk didn't work.
        """
        return json.loads(self._run_cmd_check_output(
            self._build_gcloud_list_cloud_scheduler_jobs_args()))


    def _run_cmd(self, args):
        """ Runs a shell command. """
        logging.info("Running command: %s", ' '.join(args))
        subprocess.check_call(args)


    def _run_cmd_check_output(self, args):
        logging.info("Running command: %s", ' '.join(args))
        """ runs a shell command and returns output """
        return subprocess.check_output(args).decode('utf-8')


    def _gcp_cloud_scheduler_job_to_stallion_cloud_scheduler_job(
            self,
            gcp_csj):
        # NOTE: to Stallion, these are the only facts that matter. This
        # conversion upon load makes the comparison to jobs defined in the
        # config easier.
        stallion_csj = {
            'name': self._strip_job_group_prefix(
                self._format_local_job_name(gcp_csj['name'])),
            'schedule': gcp_csj['schedule'],
            'pub_sub_target': {
                'data': base64.b64decode(gcp_csj['pubsubTarget']['data']).decode('utf-8'),
                'topic_name': self._format_local_topic_name(
                    gcp_csj['pubsubTarget']['topicName'])
            },
            'min_backoff': int(gcp_csj['retryConfig']['minBackoffDuration'][:-1]),
            'max_backoff': int(gcp_csj['retryConfig']['maxBackoffDuration'][:-1])
        }

        if 'retryCount' in gcp_csj['retryConfig']:
            stallion_csj['max_retry_attempts'] = gcp_csj['retryConfig']['retryCount']

        return stallion_csj
